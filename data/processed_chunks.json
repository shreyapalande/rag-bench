[
  {
    "chunk_id": "doc1_0",
    "file_name": "doc1.pdf",
    "text": "Top 50 Large Language Model (LLM) Interview\nQuestions\nHao Hoang - Follow me on LinkedIn for AI insights!\nMay 2025\nExplore the key concepts, techniques, and challenges of Large Language Models (LLMs)\nwith this comprehensive guide, crafted for AI enthusiasts and professionals preparing for\ninterviews.\nIntroduction\nLarge Language Models (LLMs) are revolutionizing artiﬁcial intelligence, enabling ap-\nplications from chatbots to automated content creation.\nThis document compiles 50\nessential interview questions, carefully curated to deepen your understanding of LLMs.\nEach question is paired with a detailed answer, blending technical insights with practical\nexamples. Share this knowledge with your network to spark meaningful discussions in\nthe AI community!\n1\nQuestion 1: What does tokenization e",
    "chunk_index": 0
  },
  {
    "chunk_id": "doc1_1",
    "file_name": "doc1.pdf",
    "text": "practical\nexamples. Share this knowledge with your network to spark meaningful discussions in\nthe AI community!\n1\nQuestion 1: What does tokenization entail, and why is it\ncritical for LLMs?\nTokenization involves breaking down text into smaller units, or tokens, such as words,\nsubwords, or characters. For example, \"artiﬁcial\" might be split into \"art,\" \"iﬁc,\" and\n\"ial.\" This process is vital because LLMs process numerical representations of tokens,\nnot raw text. Tokenization enables models to handle diverse languages, manage rare or\nunknown words, and optimize vocabulary size, enhancing computational eﬃciency and\nmodel performance.\n2\nQuestion 2: How does the attention mechanism function in\ntransformer models?\nThe attention mechanism allows LLMs to weigh the importance of diﬀerent tokens in ",
    "chunk_index": 1
  },
  {
    "chunk_id": "doc1_2",
    "file_name": "doc1.pdf",
    "text": "2: How does the attention mechanism function in\ntransformer models?\nThe attention mechanism allows LLMs to weigh the importance of diﬀerent tokens in a se-\nquence when generating or interpreting text. It computes similarity scores between query,\nkey, and value vectors, using operations like dot products, to focus on relevant tokens.\nFor instance, in \"The cat chased the mouse,\" attention helps the model link \"mouse\" to\n\"chased.\" This mechanism improves context understanding, making transformers highly\neﬀective for NLP tasks.\n1\n3\nQuestion 3: What is the context window in LLMs, and why\ndoes it matter?\nThe context window refers to the number of tokens an LLM can process at once, deﬁning\nits \"memory\" for understanding or generating text. A larger window, like 32,000 tokens,\nallows the model to ",
    "chunk_index": 2
  },
  {
    "chunk_id": "doc1_3",
    "file_name": "doc1.pdf",
    "text": "okens an LLM can process at once, deﬁning\nits \"memory\" for understanding or generating text. A larger window, like 32,000 tokens,\nallows the model to consider more context, improving coherence in tasks like summariza-\ntion. However, it increases computational costs. Balancing window size with eﬃciency is\ncrucial for practical LLM deployment.\n4\nQuestion 4: What distinguishes LoRA from QLoRA in ﬁne-\ntuning LLMs?\nLoRA (Low-Rank Adaptation) is a ﬁne-tuning method that adds low-rank matrices to\na models layers, enabling eﬃcient adaptation with minimal memory overhead. QLoRA\nextends this by applying quantization (e.g., 4-bit precision) to further reduce memory\nusage while maintaining accuracy. For example, QLoRA can ﬁne-tune a 70B-parameter\nmodel on a single GPU, making it ideal for resource-con",
    "chunk_index": 3
  },
  {
    "chunk_id": "doc1_4",
    "file_name": "doc1.pdf",
    "text": "educe memory\nusage while maintaining accuracy. For example, QLoRA can ﬁne-tune a 70B-parameter\nmodel on a single GPU, making it ideal for resource-constrained environments.\n5\nQuestion 5: How does beam search improve text generation\ncompared to greedy decoding?\nBeam search explores multiple word sequences during text generation, keeping the top\nk candidates (beams) at each step, unlike greedy decoding, which selects only the most\nprobable word. This approach, with k = 5, for instance, ensures more coherent outputs\nby balancing probability and diversity, especially in tasks like machine translation or\ndialogue generation.\n6\nQuestion 6: What role does temperature play in controlling\nLLM output?\nTemperature is a hyperparameter that adjusts the randomness of token selection in text\ngeneration. ",
    "chunk_index": 4
  },
  {
    "chunk_id": "doc1_5",
    "file_name": "doc1.pdf",
    "text": "e does temperature play in controlling\nLLM output?\nTemperature is a hyperparameter that adjusts the randomness of token selection in text\ngeneration. A low temperature (e.g., 0.3) favors high-probability tokens, producing pre-\ndictable outputs.\nA high temperature (e.g., 1.5) increases diversity by ﬂattening the\nprobability distribution. Setting temperature to 0.8 often balances creativity and coher-\nence for tasks like storytelling.\n7\nQuestion 7: What is masked language modeling, and how\ndoes it aid pretraining?\nMasked language modeling (MLM) involves hiding random tokens in a sequence and\ntraining the model to predict them based on context. Used in models like BERT, MLM\nfosters bidirectional understanding of language, enabling the model to grasp semantic\n2\nrelationships. This pretraining ",
    "chunk_index": 5
  },
  {
    "chunk_id": "doc1_6",
    "file_name": "doc1.pdf",
    "text": "Used in models like BERT, MLM\nfosters bidirectional understanding of language, enabling the model to grasp semantic\n2\nrelationships. This pretraining approach equips LLMs for tasks like sentiment analysis\nor question answering.\n8\nQuestion 8: What are sequence-to-sequence models, and where\nare they applied?\nSequence-to-sequence (Seq2Seq) models transform an input sequence into an output se-\nquence, often of diﬀerent lengths. They consist of an encoder to process the input and a\ndecoder to generate the output. Applications include machine translation (e.g., English\nto Spanish), text summarization, and chatbots, where variable-length inputs and outputs\nare common.\n9\nQuestion 9: How do autoregressive and masked models diﬀer\nin LLM training?\nAutoregressive models, like GPT, predict tokens seque",
    "chunk_index": 6
  },
  {
    "chunk_id": "doc1_7",
    "file_name": "doc1.pdf",
    "text": "outputs\nare common.\n9\nQuestion 9: How do autoregressive and masked models diﬀer\nin LLM training?\nAutoregressive models, like GPT, predict tokens sequentially based on prior tokens, ex-\ncelling in generative tasks such as text completion. Masked models, like BERT, predict\nmasked tokens using bidirectional context, making them ideal for understanding tasks\nlike classiﬁcation. Their training objectives shape their strengths in generation versus\ncomprehension.\n10\nQuestion 10: What are embeddings, and how are they ini-\ntialized in LLMs?\nEmbeddings are dense vectors that represent tokens in a continuous space, capturing\nsemantic and syntactic properties. They are often initialized randomly or with pretrained\nmodels like GloVe, then ﬁne-tuned during training. For example, the embedding for \"dog\"\n",
    "chunk_index": 7
  },
  {
    "chunk_id": "doc1_8",
    "file_name": "doc1.pdf",
    "text": "rties. They are often initialized randomly or with pretrained\nmodels like GloVe, then ﬁne-tuned during training. For example, the embedding for \"dog\"\nmight evolve to reﬂect its context in pet-related tasks, enhancing model accuracy.\n11\nQuestion 11: What is next sentence prediction, and how\ndoes it enhance LLMs?\nNext sentence prediction (NSP) trains models to determine if two sentences are consec-\nutive or unrelated. During pretraining, models like BERT learn to classify 50% posi-\ntive (sequential) and 50% negative (random) sentence pairs. NSP improves coherence\nin tasks like dialogue systems or document summarization by understanding sentence\nrelationships.\n3\n12\nQuestion 12: How do top-k and top-p sampling diﬀer in text\ngeneration?\nTop-k sampling selects the k most probable tokens (e.g., k",
    "chunk_index": 8
  },
  {
    "chunk_id": "doc1_9",
    "file_name": "doc1.pdf",
    "text": "\nrelationships.\n3\n12\nQuestion 12: How do top-k and top-p sampling diﬀer in text\ngeneration?\nTop-k sampling selects the k most probable tokens (e.g., k = 20) for random sampling,\nensuring controlled diversity. Top-p (nucleus) sampling chooses tokens whose cumulative\nprobability exceeds a threshold p (e.g., 0.95), adapting to context. Top-p oﬀers more\nﬂexibility, producing varied yet coherent outputs in creative writing.\n13\nQuestion 13: Why is prompt engineering crucial for LLM\nperformance?\nPrompt engineering involves designing inputs to elicit desired LLM responses. A clear\nprompt, like \"Summarize this article in 100 words,\" improves output relevance compared\nto vague instructions. Its especially eﬀective in zero-shot or few-shot settings, enabling\nLLMs to tackle tasks like translation or c",
    "chunk_index": 9
  },
  {
    "chunk_id": "doc1_10",
    "file_name": "doc1.pdf",
    "text": "levance compared\nto vague instructions. Its especially eﬀective in zero-shot or few-shot settings, enabling\nLLMs to tackle tasks like translation or classiﬁcation without extensive ﬁne-tuning.\n14\nQuestion 14: How can LLMs avoid catastrophic forgetting\nduring ﬁne-tuning?\nCatastrophic forgetting occurs when ﬁne-tuning erases prior knowledge. Mitigation strate-\ngies include:\n• Rehearsal: Mixing old and new data during training.\n• Elastic Weight Consolidation: Prioritizing critical weights to preserve knowledge.\n• Modular Architectures: Adding task-speciﬁc modules to avoid overwriting.\nThese methods ensure LLMs retain versatility across tasks.\n15\nQuestion 15: What is model distillation, and how does it\nbeneﬁt LLMs?\nModel distillation trains a smaller \"student\" model to mimic a larger \"teacher\"",
    "chunk_index": 10
  },
  {
    "chunk_id": "doc1_11",
    "file_name": "doc1.pdf",
    "text": "\nQuestion 15: What is model distillation, and how does it\nbeneﬁt LLMs?\nModel distillation trains a smaller \"student\" model to mimic a larger \"teacher\" models\noutputs, using soft probabilities rather than hard labels. This reduces memory and com-\nputational requirements, enabling deployment on devices like smartphones while retaining\nnear-teacher performance, ideal for real-time applications.\n16\nQuestion 16: How do LLMs manage out-of-vocabulary (OOV)\nwords?\nLLMs use subword tokenization, like Byte-Pair Encoding (BPE), to break OOV words\ninto known subword units. For instance, \"cryptocurrency\" might split into \"crypto\" and\n\"currency.\" This approach allows LLMs to process rare or new words, ensuring robust\nlanguage understanding and generation.\n4\n17\nQuestion 17: How do transformers improve on",
    "chunk_index": 11
  },
  {
    "chunk_id": "doc1_12",
    "file_name": "doc1.pdf",
    "text": "oach allows LLMs to process rare or new words, ensuring robust\nlanguage understanding and generation.\n4\n17\nQuestion 17: How do transformers improve on traditional\nSeq2Seq models?\nTransformers overcome Seq2Seq limitations by:\n• Parallel Processing: Self-attention enables simultaneous token processing, unlike\nsequential RNNs.\n• Long-Range Dependencies: Attention captures distant token relationships.\n• Positional Encodings: These preserve sequence order.\nThese features enhance scalability and performance in tasks like translation.\n18\nQuestion 18: What is overﬁtting, and how can it be miti-\ngated in LLMs?\nOverﬁtting occurs when a model memorizes training data, failing to generalize. Mitigation\nincludes:\n• Regularization: L1/L2 penalties simplify models.\n• Dropout: Randomly disables neurons dur",
    "chunk_index": 12
  },
  {
    "chunk_id": "doc1_13",
    "file_name": "doc1.pdf",
    "text": "training data, failing to generalize. Mitigation\nincludes:\n• Regularization: L1/L2 penalties simplify models.\n• Dropout: Randomly disables neurons during training.\n• Early Stopping: Halts training when validation performance plateaus.\nThese techniques ensure robust generalization to unseen data.\n19\nQuestion 19: What are generative versus discriminative mod-\nels in NLP?\nGenerative models, like GPT, model joint probabilities to create new data, such as text or\nimages. Discriminative models, like BERT for classiﬁcation, model conditional probabil-\nities to distinguish classes, e.g., sentiment analysis. Generative models excel in creation,\nwhile discriminative models focus on accurate classiﬁcation.\n20\nQuestion 20: How does GPT-4 diﬀer from GPT-3 in features\nand applications?\nGPT-4 surpasses G",
    "chunk_index": 13
  },
  {
    "chunk_id": "doc1_14",
    "file_name": "doc1.pdf",
    "text": "discriminative models focus on accurate classiﬁcation.\n20\nQuestion 20: How does GPT-4 diﬀer from GPT-3 in features\nand applications?\nGPT-4 surpasses GPT-3 with:\n• Multimodal Input: Processes text and images.\n• Larger Context: Handles up to 25,000 tokens versus GPT-3s 4,096.\n• Enhanced Accuracy: Reduces factual errors through better ﬁne-tuning.\nThese improvements expand its use in visual question answering and complex dialogues.\n5\n21\nQuestion 21: What are positional encodings, and why are\nthey used?\nPositional encodings add sequence order information to transformer inputs, as self-attention\nlacks inherent order awareness. Using sinusoidal functions or learned vectors, they ensure\ntokens like \"king\" and \"crown\" are interpreted correctly based on position, critical for\ntasks like translation.",
    "chunk_index": 14
  },
  {
    "chunk_id": "doc1_15",
    "file_name": "doc1.pdf",
    "text": "tions or learned vectors, they ensure\ntokens like \"king\" and \"crown\" are interpreted correctly based on position, critical for\ntasks like translation.\n22\nQuestion 22: What is multi-head attention, and how does\nit enhance LLMs?\nMulti-head attention splits queries, keys, and values into multiple subspaces, allowing\nthe model to focus on diﬀerent aspects of the input simultaneously. For example, in\na sentence, one head might focus on syntax, another on semantics. This improves the\nmodels ability to capture complex patterns.\n23\nQuestion 23: How is the softmax function applied in atten-\ntion mechanisms?\nThe softmax function normalizes attention scores into a probability distribution:\nsoftmax(xi) =\nexi\n∑\nj exj\nIn attention, it converts raw similarity scores (from query-key dot products) into wei",
    "chunk_index": 15
  },
  {
    "chunk_id": "doc1_16",
    "file_name": "doc1.pdf",
    "text": "ores into a probability distribution:\nsoftmax(xi) =\nexi\n∑\nj exj\nIn attention, it converts raw similarity scores (from query-key dot products) into weights,\nemphasizing relevant tokens. This ensures the model focuses on contextually important\nparts of the input.\n24\nQuestion 24: How does the dot product contribute to self-\nattention?\nIn self-attention, the dot product between query (Q) and key (K) vectors computes\nsimilarity scores:\nScore = Q · K\n√dk\nHigh scores indicate relevant tokens. While eﬃcient, its quadratic complexity (O(n2)) for\nlong sequences has spurred research into sparse attention alternatives.\n25\nQuestion 25: Why is cross-entropy loss used in language\nmodeling?\nCross-entropy loss measures the divergence between predicted and true token probabili-\nties:\nL = −\n∑\nyi log(ˆyi)\n6\nI",
    "chunk_index": 16
  },
  {
    "chunk_id": "doc1_17",
    "file_name": "doc1.pdf",
    "text": "y loss used in language\nmodeling?\nCross-entropy loss measures the divergence between predicted and true token probabili-\nties:\nL = −\n∑\nyi log(ˆyi)\n6\nIt penalizes incorrect predictions, encouraging accurate token selection. In language mod-\neling, it ensures the model assigns high probabilities to correct next tokens, optimizing\nperformance.\n26\nQuestion 26: How are gradients computed for embeddings\nin LLMs?\nGradients for embeddings are computed using the chain rule during backpropagation:\n∂L\n∂E =\n∂L\n∂logits · ∂logits\n∂E\nThese gradients adjust embedding vectors to minimize loss, reﬁning their semantic rep-\nresentations for better task performance.\n27\nQuestion 27: What is the Jacobian matrixs role in trans-\nformer backpropagation?\nThe Jacobian matrix captures partial derivatives of outputs wi",
    "chunk_index": 17
  },
  {
    "chunk_id": "doc1_18",
    "file_name": "doc1.pdf",
    "text": "ce.\n27\nQuestion 27: What is the Jacobian matrixs role in trans-\nformer backpropagation?\nThe Jacobian matrix captures partial derivatives of outputs with respect to inputs. In\ntransformers, it helps compute gradients for multidimensional outputs, ensuring accu-\nrate updates to weights and embeddings during backpropagation, critical for optimizing\ncomplex models.\n28\nQuestion 28: How do eigenvalues and eigenvectors relate to\ndimensionality reduction?\nEigenvectors deﬁne principal directions in data, and eigenvalues indicate their variance.\nIn techniques like PCA, selecting eigenvectors with high eigenvalues reduces dimension-\nality while retaining most variance, enabling eﬃcient data representation for LLMs input\nprocessing.\n29\nQuestion 29: What is KL divergence, and how is it used in\nLLMs?\nKL",
    "chunk_index": 18
  },
  {
    "chunk_id": "doc1_19",
    "file_name": "doc1.pdf",
    "text": "g most variance, enabling eﬃcient data representation for LLMs input\nprocessing.\n29\nQuestion 29: What is KL divergence, and how is it used in\nLLMs?\nKL divergence quantiﬁes the diﬀerence between two probability distributions:\nDKL(P||Q) =\n∑\nP(x) log P(x)\nQ(x)\nIn LLMs, it evaluates how closely model predictions match true distributions, guiding\nﬁne-tuning to improve output quality and alignment with target data.\n7\n30\nQuestion 30: What is the derivative of the ReLU function,\nand why is it signiﬁcant?\nThe ReLU function, f(x) = max(0, x), has a derivative:\nf ′(x) =\n{\n1\nif x > 0\n0\notherwise\nIts sparsity and non-linearity prevent vanishing gradients, making ReLU computationally\neﬃcient and widely used in LLMs for robust training.\n31\nQuestion 31: How does the chain rule apply to gradient\ndescent in",
    "chunk_index": 19
  },
  {
    "chunk_id": "doc1_20",
    "file_name": "doc1.pdf",
    "text": " making ReLU computationally\neﬃcient and widely used in LLMs for robust training.\n31\nQuestion 31: How does the chain rule apply to gradient\ndescent in LLMs?\nThe chain rule computes derivatives of composite functions:\nd\ndxf(g(x)) = f ′(g(x)) · g′(x)\nIn gradient descent, it enables backpropagation to calculate gradients layer by layer,\nupdating parameters to minimize loss eﬃciently across deep LLM architectures.\n32\nQuestion 32: How are attention scores calculated in trans-\nformers?\nAttention scores are computed as:\nAttention(Q, K, V ) = softmax\n(QKT\n√dk\n)\nV\nThe scaled dot product measures token relevance, and softmax normalizes scores to focus\non key tokens, enhancing context-aware generation in tasks like summarization.\n33\nQuestion 33: How does Gemini optimize multimodal LLM\ntraining?\nGemin",
    "chunk_index": 20
  },
  {
    "chunk_id": "doc1_21",
    "file_name": "doc1.pdf",
    "text": "on key tokens, enhancing context-aware generation in tasks like summarization.\n33\nQuestion 33: How does Gemini optimize multimodal LLM\ntraining?\nGemini enhances eﬃciency via:\n• Uniﬁed Architecture: Combines text and image processing for parameter eﬃciency.\n• Advanced Attention: Improves cross-modal learning stability.\n• Data Eﬃciency: Uses self-supervised techniques to reduce labeled data needs.\nThese features make Gemini more stable and scalable than models like GPT-4.\n8\n34\nQuestion 34: What types of foundation models exist?\nFoundation models include:\n• Language Models: BERT, GPT-4 for text tasks.\n• Vision Models: ResNet for image classiﬁcation.\n• Generative Models: DALL-E for content creation.\n• Multimodal Models: CLIP for text-image tasks.\nThese models leverage broad pretraining for div",
    "chunk_index": 21
  },
  {
    "chunk_id": "doc1_22",
    "file_name": "doc1.pdf",
    "text": "ion.\n• Generative Models: DALL-E for content creation.\n• Multimodal Models: CLIP for text-image tasks.\nThese models leverage broad pretraining for diverse applications.\n35\nQuestion 35: How does PEFT mitigate catastrophic forget-\nting?\nParameter-Eﬃcient Fine-Tuning (PEFT) updates only a small subset of parameters,\nfreezing the rest to preserve pretrained knowledge. Techniques like LoRA ensure LLMs\nadapt to new tasks without losing core capabilities, maintaining performance across do-\nmains.\n36\nQuestion 36: What are the steps in Retrieval-Augmented\nGeneration (RAG)?\nRAG involves:\n1. Retrieval: Fetching relevant documents using query embeddings.\n2. Ranking: Sorting documents by relevance.\n3. Generation: Using retrieved context to generate accurate responses.\nRAG enhances factual accuracy in t",
    "chunk_index": 22
  },
  {
    "chunk_id": "doc1_23",
    "file_name": "doc1.pdf",
    "text": "\n2. Ranking: Sorting documents by relevance.\n3. Generation: Using retrieved context to generate accurate responses.\nRAG enhances factual accuracy in tasks like question answering.\n37\nQuestion 37: How does Mixture of Experts (MoE) enhance\nLLM scalability?\nMoE uses a gating function to activate speciﬁc expert sub-networks per input, reducing\ncomputational load. For example, only 10% of a models parameters might be used per\nquery, enabling billion-parameter models to operate eﬃciently while maintaining high\nperformance.\n38\nQuestion 38: What is Chain-of-Thought (CoT) prompting,\nand how does it aid reasoning?\nCoT prompting guides LLMs to solve problems step-by-step, mimicking human reasoning.\nFor example, in math problems, it breaks down calculations into logical steps, improving\n9\naccuracy and",
    "chunk_index": 23
  },
  {
    "chunk_id": "doc1_24",
    "file_name": "doc1.pdf",
    "text": "blems step-by-step, mimicking human reasoning.\nFor example, in math problems, it breaks down calculations into logical steps, improving\n9\naccuracy and interpretability in complex tasks like logical inference or multi-step queries.\n39\nQuestion 39: How do discriminative and generative AI dif-\nfer?\nDiscriminative AI, like sentiment classiﬁers, predicts labels based on input features, mod-\neling conditional probabilities. Generative AI, like GPT, creates new data by modeling\njoint probabilities, suitable for tasks like text or image generation, oﬀering creative ﬂexi-\nbility.\n40\nQuestion 40: How does knowledge graph integration im-\nprove LLMs?\nKnowledge graphs provide structured, factual data, enhancing LLMs by:\n• Reducing Hallucinations: Verifying facts against the graph.\n• Improving Reasoning",
    "chunk_index": 24
  },
  {
    "chunk_id": "doc1_25",
    "file_name": "doc1.pdf",
    "text": "wledge graphs provide structured, factual data, enhancing LLMs by:\n• Reducing Hallucinations: Verifying facts against the graph.\n• Improving Reasoning: Leveraging entity relationships.\n• Enhancing Context: Oﬀering structured context for better responses.\nThis is valuable for question answering and entity recognition.\n41\nQuestion 41: What is zero-shot learning, and how do LLMs\nimplement it?\nZero-shot learning allows LLMs to perform untrained tasks using general knowledge from\npretraining. For example, prompted with \"Classify this review as positive or negative,\"\nan LLM can infer sentiment without task-speciﬁc data, showcasing its versatility.\n42\nQuestion 42: How does Adaptive Softmax optimize LLMs?\nAdaptive Softmax groups words by frequency, reducing computations for rare words. This\nlowers",
    "chunk_index": 25
  },
  {
    "chunk_id": "doc1_26",
    "file_name": "doc1.pdf",
    "text": "42\nQuestion 42: How does Adaptive Softmax optimize LLMs?\nAdaptive Softmax groups words by frequency, reducing computations for rare words. This\nlowers the cost of handling large vocabularies, speeding up training and inference while\nmaintaining accuracy, especially in resource-limited settings.\n43\nQuestion 43: How do transformers address the vanishing\ngradient problem?\nTransformers mitigate vanishing gradients via:\n• Self-Attention: Avoiding sequential dependencies.\n• Residual Connections: Allowing direct gradient ﬂow.\n• Layer Normalization: Stabilizing updates.\n10\nThese ensure eﬀective training of deep models, unlike RNNs.\n44\nQuestion 44: What is few-shot learning, and what are its\nbeneﬁts?\nFew-shot learning enables LLMs to perform tasks with minimal examples, leveraging\npretrained knowle",
    "chunk_index": 26
  },
  {
    "chunk_id": "doc1_27",
    "file_name": "doc1.pdf",
    "text": "at is few-shot learning, and what are its\nbeneﬁts?\nFew-shot learning enables LLMs to perform tasks with minimal examples, leveraging\npretrained knowledge. Beneﬁts include reduced data needs, faster adaptation, and cost\neﬃciency, making it ideal for niche tasks like specialized text classiﬁcation.\n45\nQuestion 45: How would you ﬁx an LLM generating biased\nor incorrect outputs?\nTo address biased or incorrect outputs:\n1. Analyze Patterns: Identify bias sources in data or prompts.\n2. Enhance Data: Use balanced datasets and debiasing techniques.\n3. Fine-Tune: Retrain with curated data or adversarial methods.\nThese steps improve fairness and accuracy.\n46\nQuestion 46: How do encoders and decoders diﬀer in trans-\nformers?\nEncoders process input sequences into abstract representations, capturing con",
    "chunk_index": 27
  },
  {
    "chunk_id": "doc1_28",
    "file_name": "doc1.pdf",
    "text": "y.\n46\nQuestion 46: How do encoders and decoders diﬀer in trans-\nformers?\nEncoders process input sequences into abstract representations, capturing context. De-\ncoders generate outputs, using encoder outputs and prior tokens. In translation, the\nencoder understands the source, and the decoder produces the target language, enabling\neﬀective Seq2Seq tasks.\n47\nQuestion 47: How do LLMs diﬀer from traditional statistical\nlanguage models?\nLLMs use transformer architectures, massive datasets, and unsupervised pretraining,\nunlike statistical models (e.g., N-grams) that rely on simpler, supervised methods. LLMs\nhandle long-range dependencies, contextual embeddings, and diverse tasks, but require\nsigniﬁcant computational resources.\n48\nQuestion 48: What is a hyperparameter, and why is it im-\nportant?\n",
    "chunk_index": 28
  },
  {
    "chunk_id": "doc1_29",
    "file_name": "doc1.pdf",
    "text": "l embeddings, and diverse tasks, but require\nsigniﬁcant computational resources.\n48\nQuestion 48: What is a hyperparameter, and why is it im-\nportant?\nHyperparameters are preset values, like learning rate or batch size, that control model\ntraining. They inﬂuence convergence and performance; for example, a high learning rate\nmay cause instability. Tuning hyperparameters optimizes LLM eﬃciency and accuracy.\n11\n49\nQuestion 49: What deﬁnes a Large Language Model (LLM)?\nLLMs are AI systems trained on vast text corpora to understand and generate human-like\nlanguage. With billions of parameters, they excel in tasks like translation, summarization,\nand question answering, leveraging contextual learning for broad applicability.\n50\nQuestion 50: What challenges do LLMs face in deployment?\nLLM challeng",
    "chunk_index": 29
  },
  {
    "chunk_id": "doc1_30",
    "file_name": "doc1.pdf",
    "text": "d question answering, leveraging contextual learning for broad applicability.\n50\nQuestion 50: What challenges do LLMs face in deployment?\nLLM challenges include:\n• Resource Intensity: High computational demands.\n• Bias: Risk of perpetuating training data biases.\n• Interpretability: Complex models are hard to explain.\n• Privacy: Potential data security concerns.\nAddressing these ensures ethical and eﬀective LLM use.\nConclusion\nThis guide equips you with in-depth knowledge of LLMs, from core concepts to advanced\ntechniques. Share it with your LinkedIn community to inspire and educate aspiring AI\nprofessionals. For more AI/ML insights, connect with me at Your LinkedIn Proﬁle.\n12\n",
    "chunk_index": 30
  },
  {
    "chunk_id": "doc1_31",
    "file_name": "doc1.pdf",
    "text": "ith me at Your LinkedIn Proﬁle.\n12\n",
    "chunk_index": 31
  },
  {
    "chunk_id": "doc2_0",
    "file_name": "doc2.pdf",
    "text": "Transformer Interview Questions: Part 1\nSagar Sudhakara\nDecember 2024\nGeneral Transformer Architecture\n1. What are the key components of a Transformer model?\nThe key components of a Transformer model are:\n• Self-Attention Mechanism: Captures relationships between tokens regardless of their positions\nin the sequence.\n• Multi-Head Attention: Allows the model to focus on different parts of the input simultaneously.\n• Feedforward Neural Network (FFN): Processes each token independently after the attention\nmechanism.\n• Positional Encoding: Introduces information about token positions in the sequence.\n• Layer Normalization: Stabilizes training and accelerates convergence.\n• Residual Connections: Helps gradient flow and mitigates vanishing gradient issues.\n• Softmax: Normalizes attention scores t",
    "chunk_index": 0
  },
  {
    "chunk_id": "doc2_1",
    "file_name": "doc2.pdf",
    "text": "accelerates convergence.\n• Residual Connections: Helps gradient flow and mitigates vanishing gradient issues.\n• Softmax: Normalizes attention scores to probabilities.\n2. Why do Transformers replace recurrence with attention mechanisms?\nTransformers replace recurrence with attention mechanisms for several reasons:\n• Parallelization: Attention mechanisms process all tokens simultaneously, allowing for much faster\ncomputation compared to sequential recurrence.\n• Better Long-Range Dependency Handling: Attention directly connects all tokens, regardless\nof their distance, whereas RNNs struggle with long-term dependencies due to vanishing gradients.\n• Scalability: Attention scales more efficiently to large datasets and sequences by leveraging GPUs\nand TPUs effectively.\n3. How is positional encodi",
    "chunk_index": 1
  },
  {
    "chunk_id": "doc2_2",
    "file_name": "doc2.pdf",
    "text": "\n• Scalability: Attention scales more efficiently to large datasets and sequences by leveraging GPUs\nand TPUs effectively.\n3. How is positional encoding implemented in Transformer models, and why\nis it necessary?\nImplementation: Positional encodings are typically implemented as sinusoidal functions of token po-\nsitions. For a position pos and dimension i:\nPE(pos,2i) = sin\n\u0010\npos\n100002i/d\n\u0011\nPE(pos,2i+1) = cos\n\u0010\npos\n100002i/d\n\u0011\nHere, d is the dimensionality of the model.\nNecessity: Since attention mechanisms are permutation-invariant, positional encoding provides infor-\nmation about the sequence order, which is critical for tasks involving ordered data.\n1\n4. Can you explain the role of the encoder and decoder in the original Trans-\nformer architecture?\nEncoder: Processes the input sequence a",
    "chunk_index": 2
  },
  {
    "chunk_id": "doc2_3",
    "file_name": "doc2.pdf",
    "text": "red data.\n1\n4. Can you explain the role of the encoder and decoder in the original Trans-\nformer architecture?\nEncoder: Processes the input sequence and generates a sequence of continuous representations.\nIt\nconsists of:\n• Multiple layers of self-attention and feedforward networks.\n• Focuses solely on understanding the input data.\nDecoder: Generates the output sequence while attending to the encoder’s representations. It consists\nof:\n• Self-attention to capture relationships in the target sequence.\n• Encoder-decoder attention to align the target output with the input.\n• Feedforward layers to refine predictions.\n5. What are the advantages of Transformers over RNNs and CNNs for NLP\ntasks?\n• Parallelization: Transformers process all tokens simultaneously, unlike RNNs, which are sequen-\ntial.\n",
    "chunk_index": 3
  },
  {
    "chunk_id": "doc2_4",
    "file_name": "doc2.pdf",
    "text": "ransformers over RNNs and CNNs for NLP\ntasks?\n• Parallelization: Transformers process all tokens simultaneously, unlike RNNs, which are sequen-\ntial.\n• Long-Range Dependencies: Attention allows better modeling of distant relationships compared\nto RNNs and CNNs.\n• Scalability: Transformers scale efficiently with data and model size.\n• Expressiveness: Multi-head attention captures diverse aspects of token relationships.\n• Robustness: No vanishing gradient issues that plague RNNs.\n6. How do Transformers handle long-range dependencies?\nTransformers use the self-attention mechanism, which assigns weights to all tokens in the input\nsequence based on their relevance to the current token. This mechanism directly connects distant tokens,\neffectively modeling long-range dependencies without relying ",
    "chunk_index": 4
  },
  {
    "chunk_id": "doc2_5",
    "file_name": "doc2.pdf",
    "text": "n their relevance to the current token. This mechanism directly connects distant tokens,\neffectively modeling long-range dependencies without relying on iterative steps.\n7. What is the role of feedforward networks in the Transformer block?\nFeedforward networks (FFNs) apply a non-linear transformation to the output of the attention mecha-\nnism for each token independently. Their role includes:\n• Refinement: Adds additional expressiveness and complexity to the model.\n• Dimensionality Transformation: Helps the model handle high-dimensional inputs effectively.\n• Token-Wise Processing: Processes each token independently, complementing the relational fo-\ncus of the attention mechanism.\nSelf-Attention Mechanism\n1. What is self-attention, and how does it differ from traditional attention\nmechanism",
    "chunk_index": 5
  },
  {
    "chunk_id": "doc2_6",
    "file_name": "doc2.pdf",
    "text": "al fo-\ncus of the attention mechanism.\nSelf-Attention Mechanism\n1. What is self-attention, and how does it differ from traditional attention\nmechanisms?\nSelf-Attention: Computes the attention score of each token in a sequence with respect to all other\ntokens, including itself. This allows the model to understand relationships between all tokens in a single\nsequence.\nTraditional Attention: Computes attention scores between tokens in one sequence (e.g., the target)\nand another sequence (e.g., the source). It is typically used for sequence-to-sequence tasks like machine\ntranslation.\nKey Difference:\n2\n• Scope: Self-attention is intra-sequence (same sequence), while traditional attention is inter-sequence\n(between different sequences).\n• Purpose: Self-attention captures contextual relationships",
    "chunk_index": 6
  },
  {
    "chunk_id": "doc2_7",
    "file_name": "doc2.pdf",
    "text": "me sequence), while traditional attention is inter-sequence\n(between different sequences).\n• Purpose: Self-attention captures contextual relationships within a sequence, whereas traditional\nattention aligns input and output sequences.\n2. How is the scaled dot-product attention calculated?\nScaled dot-product attention is computed as follows:\n1. Input: Query (Q), Key (K), and Value (V ) matrices.\n2. Dot-Product: Compute attention scores by taking the dot product of Q and K⊤:\nScores = QK⊤\n3. Scaling: Scale the scores by the square root of the key dimension (dk) to stabilize gradients:\nScaled Scores = QK⊤\n√dk\n4. Softmax: Normalize the scores across each row to form a probability distribution:\nAttention Weights = softmax\n\u0012QK⊤\n√dk\n\u0013\n5. Weighted Sum: Multiply the attention weights by V :\nOutput =",
    "chunk_index": 7
  },
  {
    "chunk_id": "doc2_8",
    "file_name": "doc2.pdf",
    "text": "ss each row to form a probability distribution:\nAttention Weights = softmax\n\u0012QK⊤\n√dk\n\u0013\n5. Weighted Sum: Multiply the attention weights by V :\nOutput = Attention Weights · V\n3. Why is the scaling factor (1/√dk) used in self-attention?\nThe scaling factor is used to:\n• Prevent Overly Large Dot-Products: When the dimensionality (dk) of the key and query\nvectors is large, their dot-products can grow excessively, leading to very sharp gradients.\n• Stabilize Softmax: Scaling the dot-product ensures the inputs to the softmax function are within\na reasonable range, leading to smoother and more numerically stable probabilities.\n4. What are the steps involved in computing multi-head attention?\n1. Linear Transformations: Transform the input sequence into multiple Q, K, and V matrices\nusing learned wei",
    "chunk_index": 8
  },
  {
    "chunk_id": "doc2_9",
    "file_name": "doc2.pdf",
    "text": "volved in computing multi-head attention?\n1. Linear Transformations: Transform the input sequence into multiple Q, K, and V matrices\nusing learned weight matrices. Each head uses a different set of weights.\n2. Scaled Dot-Product Attention: Compute attention scores for each head independently.\n3. Concatenate Outputs: Combine the outputs of all heads into a single matrix.\n4. Final Linear Transformation: Apply a learned linear transformation to the concatenated output\nto produce the final multi-head attention output.\n5. How does self-attention help in parallelizing computations in Transformers?\nSelf-attention allows for parallelization because:\n• All tokens in the sequence are processed simultaneously rather than sequentially.\n• Attention scores between all token pairs are computed in a singl",
    "chunk_index": 9
  },
  {
    "chunk_id": "doc2_10",
    "file_name": "doc2.pdf",
    "text": "• All tokens in the sequence are processed simultaneously rather than sequentially.\n• Attention scores between all token pairs are computed in a single matrix operation using Q, K, and\nV .\n• This eliminates the need for iterative computations, as required in RNNs, enabling efficient utiliza-\ntion of modern hardware like GPUs and TPUs.\n3\n6.\nWhy do we use masking in self-attention layers, especially in decoder\nblocks?\nMasking is used for two main reasons:\n• Prevent Information Leakage (Decoder):\n– In sequence generation tasks, future tokens should not influence the prediction of the current\ntoken.\n– A causal mask ensures that attention weights for future tokens are set to zero.\n• Handle Padding Tokens:\n– Padding tokens are added to ensure consistent sequence lengths.\n– A padding mask ensures",
    "chunk_index": 10
  },
  {
    "chunk_id": "doc2_11",
    "file_name": "doc2.pdf",
    "text": "for future tokens are set to zero.\n• Handle Padding Tokens:\n– Padding tokens are added to ensure consistent sequence lengths.\n– A padding mask ensures that padding tokens do not contribute to the attention computation\nby setting their weights to zero.\nMathematics of Query, Key, and Value\n1. What are queries, keys, and values in the context of Transformers?\nIn the context of Transformers:\n• Query (Q): Represents the vector that is used to ”ask” for information. It is derived from the\ncurrent token’s representation and is compared against all other tokens.\n• Key (K): Represents the vector that holds the information to be retrieved. It is associated with\neach token and is used to match the query.\n• Value (V ): Represents the actual information content associated with each token. After the que",
    "chunk_index": 11
  },
  {
    "chunk_id": "doc2_12",
    "file_name": "doc2.pdf",
    "text": "ated with\neach token and is used to match the query.\n• Value (V ): Represents the actual information content associated with each token. After the query\nis matched with the key, the value is returned as the relevant output.\nThese vectors are derived from the input sequence using learned projections (weights).\n2. How is the attention score between two tokens computed?\nThe attention score between two tokens is computed using the dot product of the query and key vectors:\nAttention Score = Qi · K⊤\nj\nWhere:\n• Qi is the query vector for token i,\n• Kj is the key vector for token j,\n• · represents the dot product.\nThis score indicates how much token i should attend to token j.\n3. Why is the softmax function applied to the attention scores?\nThe softmax function is applied to the attention scores to",
    "chunk_index": 12
  },
  {
    "chunk_id": "doc2_13",
    "file_name": "doc2.pdf",
    "text": "i should attend to token j.\n3. Why is the softmax function applied to the attention scores?\nThe softmax function is applied to the attention scores to:\n• Normalize the Scores: Softmax converts the raw attention scores into a probability distribution,\nensuring that they sum to 1. This enables the model to focus on relevant tokens more strongly\nwhile suppressing less important ones.\n• Emphasize Relevant Tokens: By applying softmax, the model can focus on the most relevant\ntokens (higher attention weights) and diminish the impact of irrelevant tokens (lower attention\nweights).\nThe softmax formula for attention scores is:\nAttention Weights = softmax\n \nQi · K⊤\nj\n√dk\n!\nwhere √dk is the scaling factor, as discussed earlier.\n4\n4. Derive the self-attention mechanism mathematically using matrix oper",
    "chunk_index": 13
  },
  {
    "chunk_id": "doc2_14",
    "file_name": "doc2.pdf",
    "text": "max\n \nQi · K⊤\nj\n√dk\n!\nwhere √dk is the scaling factor, as discussed earlier.\n4\n4. Derive the self-attention mechanism mathematically using matrix opera-\ntions.\nLet’s break down the self-attention mechanism:\n1. Input: An input sequence of tokens {x1, x2, ..., xn}, where each token is represented by a vector\nin Rd.\n2. Linear Projections: For each token, compute the query, key, and value vectors through learned\nweight matrices WQ, WK, WV :\nQ = XWQ,\nK = XWK,\nV = XWV\nWhere X is the matrix of input token embeddings, and WQ, WK, WV are learned weight matrices.\n3. Scaled Dot-Product Attention: Compute the attention scores between all tokens:\nA = QK⊤\n√dk\nWhere A is the matrix of attention scores.\n4. Apply Softmax: Normalize the attention scores using softmax:\nAttention Weights = softmax(A)\n5. Compu",
    "chunk_index": 14
  },
  {
    "chunk_id": "doc2_15",
    "file_name": "doc2.pdf",
    "text": "√dk\nWhere A is the matrix of attention scores.\n4. Apply Softmax: Normalize the attention scores using softmax:\nAttention Weights = softmax(A)\n5. Compute Output: Multiply the attention weights by the value matrix V to obtain the final\noutput:\nO = Attention Weights · V\nThe resulting matrix O is the output of the self-attention mechanism.\n5. What happens if the query and key vectors are not aligned in dimension-\nality?\nIf the query and key vectors are not aligned in dimensionality (i.e., their dimensions dq and dk are not\nthe same), the dot product operation Qi · K⊤\nj is not well-defined. Therefore, the model cannot compute\nthe attention scores properly.\nTo address this issue:\n• The dimensions of Q and K are usually made equal through learned projections (i.e., using weight\nmatrices WQ and WK",
    "chunk_index": 15
  },
  {
    "chunk_id": "doc2_16",
    "file_name": "doc2.pdf",
    "text": "properly.\nTo address this issue:\n• The dimensions of Q and K are usually made equal through learned projections (i.e., using weight\nmatrices WQ and WK to map both to the same dimensionality).\n• If dq ̸= dk, we project the query and key vectors into a shared space of dimension dcommon to\nperform the dot product.\n6.\nHow does multi-head attention improve the model’s ability to capture\ndiverse relationships in the input sequence?\nMulti-head attention improves the model by allowing it to:\n• Capture Different Aspects of Relationships: Each attention head learns different attention\npatterns, capturing various relationships in the input sequence. For example, one head might focus\non syntactic dependencies, while another might focus on semantic relationships.\n• Increase Capacity: By using multiple ",
    "chunk_index": 16
  },
  {
    "chunk_id": "doc2_17",
    "file_name": "doc2.pdf",
    "text": " example, one head might focus\non syntactic dependencies, while another might focus on semantic relationships.\n• Increase Capacity: By using multiple heads, the model has the capacity to attend to different\nparts of the input in parallel, improving its ability to capture diverse relationships.\n• Enhanced Representation: Combining the outputs of multiple heads enables the model to\naggregate a richer representation of the input sequence, which is more expressive than using a\nsingle attention mechanism.\n5\n7. What is the role of learned projections for queries, keys, and values?\nThe learned projections for queries, keys, and values serve to:\n• Transform Input Representations: The input tokens are transformed into different spaces for\nqueries, keys, and values. These projections are learned dur",
    "chunk_index": 17
  },
  {
    "chunk_id": "doc2_18",
    "file_name": "doc2.pdf",
    "text": "ansform Input Representations: The input tokens are transformed into different spaces for\nqueries, keys, and values. These projections are learned during training, allowing the model to\nadapt to the best representations for each type of vector.\n• Ensure Compatibility: The projections ensure that queries and keys are in the same space,\nmaking the dot-product operation well-defined. Similarly, the value projections enable the model\nto generate meaningful output from the attention weights.\n• Increase Model Expressiveness: By using different learned projections for each of Q, K, and\nV , the model can learn diverse ways to represent the same input tokens and their interactions,\nimproving its ability to capture complex relationships in the data.\n6\n",
    "chunk_index": 18
  },
  {
    "chunk_id": "doc2_19",
    "file_name": "doc2.pdf",
    "text": " tokens and their interactions,\nimproving its ability to capture complex relationships in the data.\n6\n",
    "chunk_index": 19
  },
  {
    "chunk_id": "doc3_0",
    "file_name": "doc3.pdf",
    "text": "Transformer Interview Questions: Part 2\nSagar Sudhakara\nDecember 2024\nFundamentals of Key-Value Caching\n1. What is the purpose of key-value caching in Transformer-based models?\nThe purpose of key-value caching is to store the computed keys and values for tokens already processed\nduring autoregressive generation. This avoids recomputing attention scores for past tokens, improving\ninference speed and reducing redundant computations.\n2.\nHow does key-value caching improve inference performance during text\ngeneration?\nBy caching the keys and values for past tokens, the model only needs to compute attention for the newly\ngenerated token.\nThis reduces the computation from O(n2) to O(n) for each token, where n is the\nsequence length.\n3. What data is stored in the key-value cache for each Transform",
    "chunk_index": 0
  },
  {
    "chunk_id": "doc3_1",
    "file_name": "doc3.pdf",
    "text": "es the computation from O(n2) to O(n) for each token, where n is the\nsequence length.\n3. What data is stored in the key-value cache for each Transformer layer?\nEach Transformer layer stores:\n• Keys (K): A tensor representing the transformed input for all past tokens.\n• Values (V ): A tensor containing the output of the attention mechanism for all past tokens.\n4. Why is it unnecessary to recompute attention for past tokens during au-\ntoregressive generation?\nPast attention computations remain unchanged as the past sequence does not change during inference.\nStoring the keys and values ensures the attention mechanism can reuse these computations for the next\ntoken.\nMechanics of Key-Value Caching\n5. How are key-value pairs initialized and updated during inference?\nThe cache is initialized as e",
    "chunk_index": 1
  },
  {
    "chunk_id": "doc3_2",
    "file_name": "doc3.pdf",
    "text": " for the next\ntoken.\nMechanics of Key-Value Caching\n5. How are key-value pairs initialized and updated during inference?\nThe cache is initialized as empty at the start of generation. For each new token, the model computes its\nkeys and values, which are then appended to the cache for subsequent use.\n6. How is the cache used to compute attention scores for new tokens?\nThe new token’s query vector is compared with the cached keys to compute attention scores. These\nscores are then used to retrieve a weighted sum of the cached values to generate the output for the new\ntoken.\n1\n7. What structures or data formats are typically used to implement the key-\nvalue cache?\nKey-value caches are typically stored as tensors of shape:\n• Keys: [num layers, num heads, seq length, dk]\n• Values: [num layers, nu",
    "chunk_index": 2
  },
  {
    "chunk_id": "doc3_3",
    "file_name": "doc3.pdf",
    "text": "key-\nvalue cache?\nKey-value caches are typically stored as tensors of shape:\n• Keys: [num layers, num heads, seq length, dk]\n• Values: [num layers, num heads, seq length, dv]\nThese tensors are updated incrementally during generation.\n8. What challenges arise when managing key-value caches for extremely long\nsequences?\n• Memory limitations: The cache size grows linearly with sequence length and the number of\nlayers/heads.\n• Computational overhead: As sequences grow, accessing the cache for attention computation\ncan become slower.\n• Handling truncation: Strategies are needed to discard or compress parts of the cache for very\nlong sequences.\nMemory and Scalability\n9. How does key-value caching affect memory usage in Transformers?\nKey-value caching increases memory usage because the model must",
    "chunk_index": 3
  },
  {
    "chunk_id": "doc3_4",
    "file_name": "doc3.pdf",
    "text": "ory and Scalability\n9. How does key-value caching affect memory usage in Transformers?\nKey-value caching increases memory usage because the model must store all past keys and values for\nevery layer and head. This can significantly impact memory requirements, especially for large models.\n10. How does the sequence length of past tokens impact the size of the cache?\nThe cache size grows linearly with the sequence length. For a sequence of length n, the memory required\nis proportional to n × dk + n × dv, multiplied by the number of layers and heads.\n11. What are potential techniques to manage memory when using key-value\ncaching for very large models or long contexts?\n• Truncation: Drop old keys and values beyond a fixed window size.\n• Compression: Use low-rank approximations or other methods t",
    "chunk_index": 4
  },
  {
    "chunk_id": "doc3_5",
    "file_name": "doc3.pdf",
    "text": "els or long contexts?\n• Truncation: Drop old keys and values beyond a fixed window size.\n• Compression: Use low-rank approximations or other methods to compress the cache.\n• Sparse attention: Only store and compute attention for a subset of tokens.\n• Offloading: Store parts of the cache on disk or less expensive memory tiers.\n12.\nHow does the number of attention heads affect the complexity of the\ncache?\nThe cache size scales linearly with the number of heads. For h attention heads, the storage requirement\nbecomes:\nh × (seq length × dk + seq length × dv)\nThis increases both memory usage and access latency.\n2\nApplications and Optimization\n1. In which scenarios is key-value caching most beneficial?\n• Autoregressive Generation: Scenarios like text generation, machine translation, and code com-",
    "chunk_index": 5
  },
  {
    "chunk_id": "doc3_6",
    "file_name": "doc3.pdf",
    "text": " which scenarios is key-value caching most beneficial?\n• Autoregressive Generation: Scenarios like text generation, machine translation, and code com-\npletion where tokens are generated one at a time.\n• Real-time Inference: Applications requiring low latency, such as conversational AI and chatbots.\n• Long Sequences: Key-value caching is particularly helpful when generating long sequences, as it\navoids recomputation for past tokens.\n• Resource-Constrained Environments: It helps optimize compute cycles in edge devices or\nwhen using large models.\n2. How does key-value caching interact with techniques like beam search or\ntemperature sampling?\n• Beam Search:\n– Each beam maintains its own cache to track the sequence of tokens generated.\n– At every step, keys and values for all beams are updated ",
    "chunk_index": 6
  },
  {
    "chunk_id": "doc3_7",
    "file_name": "doc3.pdf",
    "text": "eam Search:\n– Each beam maintains its own cache to track the sequence of tokens generated.\n– At every step, keys and values for all beams are updated independently.\n– This increases memory requirements proportionally to the beam width.\n• Temperature Sampling:\n– Temperature sampling does not affect caching itself but determines the next token probabilis-\ntically after attention computation, which depends on the cache.\n– Cached values ensure the model efficiently scores tokens for all sampling candidates.\n3. What are potential optimization strategies for implementing efficient key-\nvalue caching?\n• Fixed-Length Caches: Limit the cache to a sliding window of recent tokens, discarding older\nkeys and values.\n• Sparse Attention: Use techniques like Longformer or BigBird to attend to subsets of t",
    "chunk_index": 7
  },
  {
    "chunk_id": "doc3_8",
    "file_name": "doc3.pdf",
    "text": "ing window of recent tokens, discarding older\nkeys and values.\n• Sparse Attention: Use techniques like Longformer or BigBird to attend to subsets of tokens,\nreducing the cache size.\n• Quantization: Store keys and values in lower precision (e.g., FP16 or INT8) to save memory.\n• Efficient Indexing: Optimize data structures for quick access to the cache, such as batched or\nhierarchical storage formats.\n• Cache Sharing: Reuse caches across similar queries, if applicable, in certain tasks like search or\ndocument generation.\n4. Can key-value caching be applied to bidirectional models like BERT, or is\nit specific to autoregressive models?\n• Bidirectional Models (e.g., BERT):\n– Key-value caching is not commonly used because bidirectional models process all tokens si-\nmultaneously and do not genera",
    "chunk_index": 8
  },
  {
    "chunk_id": "doc3_9",
    "file_name": "doc3.pdf",
    "text": "ional Models (e.g., BERT):\n– Key-value caching is not commonly used because bidirectional models process all tokens si-\nmultaneously and do not generate tokens incrementally.\n– If used, caching would primarily serve tasks like speeding up intermediate representations in\niterative refinement tasks.\n• Autoregressive Models:\n– Key-value caching is specifically designed for incremental generation tasks, where each token\ndepends on previous tokens.\n– It is crucial for models like GPT where past context directly impacts the next token prediction.\n3\nUsing Transformers for GPT Models\n1. How does the Transformer architecture differ in GPT models compared to\nthe original Transformer?\n• Decoder-Only Architecture: In the original Transformer, both the encoder and decoder are\nused, with the encoder pro",
    "chunk_index": 9
  },
  {
    "chunk_id": "doc3_10",
    "file_name": "doc3.pdf",
    "text": "red to\nthe original Transformer?\n• Decoder-Only Architecture: In the original Transformer, both the encoder and decoder are\nused, with the encoder processing the input and the decoder generating the output. However, in\nGPT models (Generative Pretrained Transformers), only the decoder stack of the Transformer\nis used. This is because GPT is designed for autoregressive text generation, where the model\ngenerates one token at a time, conditioned on previously generated tokens.\n• Masked Attention: In GPT, the decoder uses unidirectional (causal) attention, meaning\neach token can only attend to earlier tokens (including itself), preventing future tokens from influ-\nencing the prediction. In contrast, the original Transformer allows both the encoder and decoder\nto attend to all tokens in the sequ",
    "chunk_index": 10
  },
  {
    "chunk_id": "doc3_11",
    "file_name": "doc3.pdf",
    "text": "okens from influ-\nencing the prediction. In contrast, the original Transformer allows both the encoder and decoder\nto attend to all tokens in the sequence, enabling bidirectional context.\n2. What modifications are made in GPT to support autoregressive text gen-\neration?\n• Unidirectional (Causal) Attention: To ensure that text generation is autoregressive, GPT\nemploys masked self-attention, which ensures that each token only attends to previous tokens in\nthe sequence (including itself) during training and inference. This modification prevents the model\nfrom peeking at future tokens, making the generation process unidirectional and maintaining the\ncausal flow of language.\n• Positional Encoding: Since the input sequence is processed one token at a time, positional\nencodings are added to the i",
    "chunk_index": 11
  },
  {
    "chunk_id": "doc3_12",
    "file_name": "doc3.pdf",
    "text": "the\ncausal flow of language.\n• Positional Encoding: Since the input sequence is processed one token at a time, positional\nencodings are added to the input embeddings to inject information about the relative positions\nof tokens in the sequence.\n• Autoregressive Training: During training, GPT is trained to predict the next token in the\nsequence, given the previous tokens. This allows the model to learn how to generate text step by\nstep, building on its own previous predictions.\n3. How are the hidden states of the decoder utilized in GPT models?\n• Token Predictions: In GPT, the hidden states generated by the decoder at each time step are\nused to predict the next token in the sequence. At each layer of the decoder, the output hidden state\nis passed through a linear layer followed by a softmax ",
    "chunk_index": 12
  },
  {
    "chunk_id": "doc3_13",
    "file_name": "doc3.pdf",
    "text": " predict the next token in the sequence. At each layer of the decoder, the output hidden state\nis passed through a linear layer followed by a softmax function to predict a probability distribution\nover the vocabulary for the next token.\n• Contextual Representation: The hidden states also provide contextual representations for each\ntoken. The final hidden state corresponding to the last token is typically used for downstream\ntasks like text classification or question answering when fine-tuned.\n• Autoregressive Generation: As GPT generates tokens one-by-one, the hidden state at each\ntime step contains the information necessary for generating the next token based on all previous\ntokens.\n4. Why is unidirectional attention used in GPT models?\n• Autoregressive Nature: Unidirectional (causal) att",
    "chunk_index": 13
  },
  {
    "chunk_id": "doc3_14",
    "file_name": "doc3.pdf",
    "text": "e next token based on all previous\ntokens.\n4. Why is unidirectional attention used in GPT models?\n• Autoregressive Nature: Unidirectional (causal) attention is used in GPT to ensure that the\nmodel generates text in an autoregressive manner. Each token can only attend to the tokens that\nhave been generated before it, not to future tokens. This is crucial for causality in text generation,\nwhere each token must be predicted based on the preceding tokens only, ensuring that the generated\ntext flows naturally.\n• Prevention of Information Leaks: Unidirectional attention prevents future tokens from influ-\nencing the prediction of the current token. This mirrors how natural text generation works: words\nare generated one after another, with each word being conditioned on the preceding context.\n4\n5.",
    "chunk_index": 14
  },
  {
    "chunk_id": "doc3_15",
    "file_name": "doc3.pdf",
    "text": "This mirrors how natural text generation works: words\nare generated one after another, with each word being conditioned on the preceding context.\n4\n5. What is the role of the language modeling objective in training GPT?\n• Predicting the Next Token: The training objective for GPT is language modeling, specifically\nautoregressive language modeling. In this approach, the model learns to predict the next token\nin a sequence, given all the previous tokens. The objective is to minimize the cross-entropy loss\nbetween the predicted token and the actual next token in the sequence.\n• Self-Supervised Learning: GPT uses a self-supervised learning approach, where no explicit\nlabels are needed. The model simply predicts the next token using the context of preceding tokens,\nmaking it an unsupervised task",
    "chunk_index": 15
  },
  {
    "chunk_id": "doc3_16",
    "file_name": "doc3.pdf",
    "text": "h, where no explicit\nlabels are needed. The model simply predicts the next token using the context of preceding tokens,\nmaking it an unsupervised task.\n• Training Objective: The model is trained to maximize the likelihood of the sequence of tokens\nby adjusting its parameters such that the predicted probability distribution over the vocabulary is\nas close as possible to the true distribution of the next token.\n6. Can you explain the architecture of GPT-2 or GPT-3 in terms of layers,\nparameters, and scaling strategies?\n• GPT-2 Architecture:\n– Layers: GPT-2 has a decoder-only architecture with multiple layers of Transformer de-\ncoders stacked on top of each other. The number of layers typically ranges from 12 to 48,\ndepending on the model size.\n– Parameters: GPT-2 comes in various sizes, with",
    "chunk_index": 16
  },
  {
    "chunk_id": "doc3_17",
    "file_name": "doc3.pdf",
    "text": " top of each other. The number of layers typically ranges from 12 to 48,\ndepending on the model size.\n– Parameters: GPT-2 comes in various sizes, with the largest model having 1.5 billion pa-\nrameters. The model scales up by increasing the number of layers, the size of the hidden\nstates, and the number of attention heads per layer.\n– Scaling Strategy: GPT-2 scales by increasing the number of layers, attention heads, and\nmodel dimensions.\nAs the model size increases, its performance improves on downstream\ntasks, though it also requires exponentially more computational resources.\n• GPT-3 Architecture:\n– Layers: GPT-3 follows the same decoder-only architecture as GPT-2 but with a signifi-\ncantly larger number of layers, typically 96 layers in the largest model.\n– Parameters: GPT-3 has up to 1",
    "chunk_index": 17
  },
  {
    "chunk_id": "doc3_18",
    "file_name": "doc3.pdf",
    "text": "ly architecture as GPT-2 but with a signifi-\ncantly larger number of layers, typically 96 layers in the largest model.\n– Parameters: GPT-3 has up to 175 billion parameters, making it one of the largest\nlanguage models to date.\nThis massive scale allows it to perform a wide range of tasks\nwithout fine-tuning (zero-shot learning).\n– Scaling Strategy: GPT-3 scales by adding more layers, increasing the model width (number\nof units in each layer), and expanding the attention heads. This scaling strategy increases the\nmodel’s capacity to handle more complex tasks but also increases the training and inference\ncosts.\n7. How does GPT handle context window limitations?\n• Fixed Context Window: Like most Transformer-based models, GPT has a fixed context win-\ndow defined by the maximum sequence length ",
    "chunk_index": 18
  },
  {
    "chunk_id": "doc3_19",
    "file_name": "doc3.pdf",
    "text": "ndow limitations?\n• Fixed Context Window: Like most Transformer-based models, GPT has a fixed context win-\ndow defined by the maximum sequence length that it can process at once (often 1024 tokens or\nmore, depending on the model size). This means that GPT can only ”see” a limited number of\nprevious tokens when generating the next token.\n• Context Window Expansion: In models like GPT-2 and GPT-3, the context window limitation\nis addressed by increasing the maximum sequence length as the models scale up. However, the\nquadratic scaling of self-attention (with O(T 2) complexity) limits how large the context window\ncan be for extremely long sequences.\n• Long-Range Dependencies: To handle longer context or dependencies beyond the fixed window,\ntechniques like sliding windows, sparse attention, o",
    "chunk_index": 19
  },
  {
    "chunk_id": "doc3_20",
    "file_name": "doc3.pdf",
    "text": "ces.\n• Long-Range Dependencies: To handle longer context or dependencies beyond the fixed window,\ntechniques like sliding windows, sparse attention, or memory-augmented networks could\nbe used, but these are typically more common in specialized Transformer variants (e.g., Longformer,\nReformer). GPT’s architecture remains mostly fixed in its context window size.\n5\nSummary of GPT Models:\n• Decoder-Only Architecture: GPT models use only the decoder part of the Transformer to\nenable autoregressive text generation.\n• Unidirectional Attention: GPT uses causal (unidirectional) attention to ensure each token is\ngenerated based only on the preceding tokens.\n• Autoregressive Objective: GPT’s training objective is language modeling, where it predicts the\nnext token in the sequence given the previous t",
    "chunk_index": 20
  },
  {
    "chunk_id": "doc3_21",
    "file_name": "doc3.pdf",
    "text": "kens.\n• Autoregressive Objective: GPT’s training objective is language modeling, where it predicts the\nnext token in the sequence given the previous tokens.\n• Architecture Scaling: GPT models scale by increasing the number of layers, the size of hidden\nstates, and attention heads, with GPT-3 reaching up to 175 billion parameters.\n• Context Window: GPT handles context window limitations by using a fixed sequence length\nand increasing the model size, but still faces challenges with long-range dependencies in sequences.\nUsing Transformers for BERT Models\n1. How does BERT differ from GPT in terms of training objectives?\n• GPT’s Training Objective: GPT (Generative Pretrained Transformer) uses autoregressive\nlanguage modeling, where it predicts the next token in a sequence based on the previous ",
    "chunk_index": 21
  },
  {
    "chunk_id": "doc3_22",
    "file_name": "doc3.pdf",
    "text": ": GPT (Generative Pretrained Transformer) uses autoregressive\nlanguage modeling, where it predicts the next token in a sequence based on the previous tokens.\nIt learns to generate text one token at a time in a left-to-right manner, relying on unidirectional\n(causal) attention.\n• BERT’s Training Objective: BERT (Bidirectional Encoder Representations from Transformers)\nuses a masked language modeling (MLM) approach, where some tokens in the input sequence\nare randomly replaced with a [MASK] token. The model’s goal is to predict the original value of\nthese masked tokens based on both the left and right context, enabling bidirectional understanding.\nBERT also uses next sentence prediction (NSP) to predict whether two consecutive sentences\nin a pair are logically consecutive.\nThus, while GPT is",
    "chunk_index": 22
  },
  {
    "chunk_id": "doc3_23",
    "file_name": "doc3.pdf",
    "text": "ng.\nBERT also uses next sentence prediction (NSP) to predict whether two consecutive sentences\nin a pair are logically consecutive.\nThus, while GPT is trained to generate text, BERT is trained to understand text by leveraging bidi-\nrectional context.\n2. What is masked language modeling (MLM), and how is it used in BERT\ntraining?\n• MLM Explanation: In masked language modeling (MLM), during training, random tokens\nin the input sequence are replaced with a special [MASK] token. The model is then tasked with\npredicting the original values of these masked tokens based on the surrounding context (both the\nleft and right sides).\n• Training Process: This task forces the model to learn rich, bidirectional representations be-\ncause, unlike traditional language models that rely on left-to-right or ri",
    "chunk_index": 23
  },
  {
    "chunk_id": "doc3_24",
    "file_name": "doc3.pdf",
    "text": "This task forces the model to learn rich, bidirectional representations be-\ncause, unlike traditional language models that rely on left-to-right or right-to-left processing, MLM\nenables the model to use both past and future context for understanding the meaning of a word\nin a sentence.\n• Benefit: MLM is critical for BERT as it helps the model learn deep contextual relationships\nbetween words in a sentence, rather than just focusing on sequential generation.\n3. What is next sentence prediction (NSP), and how does it benefit BERT?\n• NSP Explanation: Next sentence prediction (NSP) is a task used in BERT’s pretraining to help\nthe model understand sentence-level relationships. During training, BERT is given a pair of\nsentences, and it must predict whether the second sentence logically follows t",
    "chunk_index": 24
  },
  {
    "chunk_id": "doc3_25",
    "file_name": "doc3.pdf",
    "text": " sentence-level relationships. During training, BERT is given a pair of\nsentences, and it must predict whether the second sentence logically follows the first one.\n• Training Process: For each input pair, BERT is trained to determine whether the second sentence\nis the true next sentence or a random sentence from the corpus. This binary classification task\nhelps the model understand the relationship between two sentences, a critical ability for tasks like\nquestion answering and natural language inference (NLI).\n6\n• Benefit: NSP helps BERT develop a deeper understanding of contextual dependencies and\nsentence-level relationships, which is important for tasks like sentence classification and ques-\ntion answering.\n4. Why is bidirectional attention critical for BERT’s success?\n• Bidirectional C",
    "chunk_index": 25
  },
  {
    "chunk_id": "doc3_26",
    "file_name": "doc3.pdf",
    "text": "tant for tasks like sentence classification and ques-\ntion answering.\n4. Why is bidirectional attention critical for BERT’s success?\n• Bidirectional Context: Unlike GPT, which uses unidirectional attention (only looking at\npast tokens), BERT uses bidirectional attention. This means that BERT can attend to both\nthe left and right context of each token during training, allowing it to learn a more complete and\nnuanced representation of language.\n• Improved Contextual Understanding: Bidirectional attention enables BERT to understand\nwords based on their full context, making it more effective at understanding ambiguous words,\nword meanings that change depending on context, and syntactic or semantic relations within\nsentences.\n• Crucial for NLP Tasks: This bidirectional context is particularly i",
    "chunk_index": 26
  },
  {
    "chunk_id": "doc3_27",
    "file_name": "doc3.pdf",
    "text": "ange depending on context, and syntactic or semantic relations within\nsentences.\n• Crucial for NLP Tasks: This bidirectional context is particularly important for tasks like named\nentity recognition (NER), question answering, and sentiment analysis, where understand-\ning the full context of a sentence is essential.\n5. How does BERT handle fine-tuning for downstream tasks?\n• Pretraining and Fine-Tuning: BERT is first pretrained on a large corpus using its MLM and\nNSP objectives. After pretraining, BERT can be fine-tuned on specific downstream tasks by\nadding a task-specific output layer (e.g., a classification layer for sentiment analysis or a span-\nbased layer for question answering) and retraining the model on the task’s labeled data.\n• Fine-Tuning Process: Fine-tuning involves updating a",
    "chunk_index": 27
  },
  {
    "chunk_id": "doc3_28",
    "file_name": "doc3.pdf",
    "text": " span-\nbased layer for question answering) and retraining the model on the task’s labeled data.\n• Fine-Tuning Process: Fine-tuning involves updating all of BERT’s parameters (both the pre-\ntrained ones and the newly added task-specific layer) using a smaller task-specific dataset. Fine-\ntuning can be done with relatively small amounts of labeled data because BERT has already learned\ngeneral language representations during pretraining.\n• Task Versatility: BERT’s ability to be fine-tuned on multiple tasks without retraining from\nscratch is one of its major advantages, enabling it to excel at a wide range of NLP tasks such as\nclassification, translation, question answering, and more.\n6. What is the significance of segment embeddings in BERT?\n• Segment Embeddings: Segment embeddings are used i",
    "chunk_index": 28
  },
  {
    "chunk_id": "doc3_29",
    "file_name": "doc3.pdf",
    "text": "nslation, question answering, and more.\n6. What is the significance of segment embeddings in BERT?\n• Segment Embeddings: Segment embeddings are used in BERT to differentiate between different\nsegments of text, which is particularly useful for tasks like sentence-pair classification (e.g.,\nquestion answering, natural language inference).\n• Role: BERT processes pairs of sentences in its input, and segment embeddings allow the model to\ndistinguish between the two sentences. The tokens in the first sentence are assigned one segment\nembedding (usually labeled as Segment A), while the tokens in the second sentence receive a\ndifferent embedding (labeled as Segment B).\n• Benefit: This mechanism enables BERT to handle tasks that require understanding relationships\nbetween two sentences, such as det",
    "chunk_index": 29
  },
  {
    "chunk_id": "doc3_30",
    "file_name": "doc3.pdf",
    "text": "eled as Segment B).\n• Benefit: This mechanism enables BERT to handle tasks that require understanding relationships\nbetween two sentences, such as determining if two sentences are logically related or answering a\nquestion based on a context passage.\n7. Can you explain why BERT uses [CLS] and [SEP] tokens?\n• [CLS] Token: The [CLS] (classification) token is used as a special token at the beginning of every\ninput sequence. During fine-tuning, the hidden state corresponding to the [CLS] token is typically\nused for classification tasks. For example, in sentiment analysis, the [CLS] token’s representation\nis passed through a classifier to predict the sentiment of the entire text.\n• [SEP] Token: The [SEP] (separator) token is used to separate different segments or sentences\nin a pair. It is place",
    "chunk_index": 30
  },
  {
    "chunk_id": "doc3_31",
    "file_name": "doc3.pdf",
    "text": "he sentiment of the entire text.\n• [SEP] Token: The [SEP] (separator) token is used to separate different segments or sentences\nin a pair. It is placed between sentences in tasks such as next sentence prediction (NSP) and\nsentence-pair classification. In single-sentence tasks, the [SEP] token is placed at the end of\nthe sequence.\n7\n• Benefit: These special tokens provide structural information to BERT, guiding it in handling\nsentence pairs and tasks that involve understanding the relationship between segments of text.\nSummary of BERT’s Features\n• Training Objective: BERT uses masked language modeling (MLM) and next sentence\nprediction (NSP) to pretrain the model, learning bidirectional context and sentence-level rela-\ntionships.\n• Bidirectional Attention: Unlike GPT, BERT leverages bidirec",
    "chunk_index": 31
  },
  {
    "chunk_id": "doc3_32",
    "file_name": "doc3.pdf",
    "text": "o pretrain the model, learning bidirectional context and sentence-level rela-\ntionships.\n• Bidirectional Attention: Unlike GPT, BERT leverages bidirectional attention, which allows\nit to understand words based on both left and right context.\n• Fine-Tuning: BERT is pretrained and then fine-tuned on task-specific datasets, enabling it to\nperform a wide variety of downstream NLP tasks with minimal additional training.\n• Segment Embeddings: Segment embeddings distinguish between different text segments (e.g.,\nsentences) for tasks like sentence-pair classification.\n• Special Tokens: The [CLS] token is used for classification tasks, and the [SEP] token separates\ndifferent segments in the input, helping BERT handle various types of tasks.\n8\n",
    "chunk_index": 32
  },
  {
    "chunk_id": "doc3_33",
    "file_name": "doc3.pdf",
    "text": "oken separates\ndifferent segments in the input, helping BERT handle various types of tasks.\n8\n",
    "chunk_index": 33
  },
  {
    "chunk_id": "doc4_0",
    "file_name": "doc4.pdf",
    "text": "🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ1. Explain the requirement of RAG when LLMs are already powerful. \n \nLLMs are powerful, as they are trained on large volumes of data using sophisticated \ntechniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to \nanswer queries related to the latest events or the data not present in their training \ncorpus.  \n \nRAG addresses this challenge by retrieving relevant context from external knowledge \nsources, which allows LLMs to provide accurate responses. This is why RAG is essential \nfor LLM-based applications that need to be accurate. Otherwise, LLMs alone might \nprovide you answers that are incomplete or outdated. \n \nQ2. Is RAG still rele",
    "chunk_index": 0
  },
  {
    "chunk_id": "doc4_1",
    "file_name": "doc4.pdf",
    "text": "ased applications that need to be accurate. Otherwise, LLMs alone might \nprovide you answers that are incomplete or outdated. \n \nQ2. Is RAG still relevant in the era of long context LLMs? \n \nRAG is still important even with long context LLMs. This is because long-context LLMs \nwithout RAG have three big problems: \"lost in the middle,\", high API costs, and \nincreased latency. \n \nLong-context LLMs often struggle to find the most relevant  information in large \ncontexts, which hurts the quality of generated responses. Furthermore, processing \nlengthy sequences in each API call results in high latency and high API costs. \n \nRAG addresses these issues by providing the most relevant information from external \nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses,",
    "chunk_index": 1
  },
  {
    "chunk_id": "doc4_2",
    "file_name": "doc4.pdf",
    "text": "sues by providing the most relevant information from external \nknowledge sources. So, you still need RAG to get accurate and cost-efficient responses, \neven with long context LLMs. \n \nQ3. What are the fundamental challenges of RAG systems? \n \nRAG is powerful, but it has to deal with the following challenges: \n \nScalability: Searching and retrieving from large, dynamic knowledge sources quickly \nand efficiently requires a lot of computing power and well-optimized indexing, which \ncan be expensive or take a long time. \n \nLatency - The two-step process (retrieval then generation) can cause delays, making it \nless suitable for real-time applications without careful optimization. \n \nHallucination Risk - Even with retrieval, the model might generate plausible but \nunsupported details if the retr",
    "chunk_index": 2
  },
  {
    "chunk_id": "doc4_3",
    "file_name": "doc4.pdf",
    "text": "ons without careful optimization. \n \nHallucination Risk - Even with retrieval, the model might generate plausible but \nunsupported details if the retrieved data is ambiguous or insufficient. \n \n1                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nBias and Noise - Retrieved content might carry biases, errors, or irrelevant noise from \nthe web or other sources, which can propagate into the output. \n \nQ4. What are effective strategies to reduce latency in RAG systems? \n \nCaching, embedding quantization, selective query rewriting, and selective re-ranking are \nsome of the ways to reduce RAG latency. Caching stores ret",
    "chunk_index": 3
  },
  {
    "chunk_id": "doc4_4",
    "file_name": "doc4.pdf",
    "text": " \nCaching, embedding quantization, selective query rewriting, and selective re-ranking are \nsome of the ways to reduce RAG latency. Caching stores retrieved results or generated \nresponses to avoid redundant computation. Embedding quantization to lower bit \nprecision reduces memory and computational load, speeding up retrieval.  \n \nSelective query rewriting enhances recall and relevance by refining queries prior to \nretrieval, primarily utilized for complex or ambiguous queries. Selective re-ranking is \nonly used for complicated queries, which cuts down on unnecessary computation for \nsimpler ones.  \n \nQ5. Explain R, A, and G in RAG.  \n \nRAG stands for Retrieval-Augmented Generation. The \"R\" or Retrieval, refers to the \nprocess of searching and fetching the most relevant information from e",
    "chunk_index": 4
  },
  {
    "chunk_id": "doc4_5",
    "file_name": "doc4.pdf",
    "text": "stands for Retrieval-Augmented Generation. The \"R\" or Retrieval, refers to the \nprocess of searching and fetching the most relevant information from external \nknowledge sources for the given user query.  \n \nThe \"A\" or Augmented, involves including the retrieved relevant context in the LLM \nprompt having the user query and instructions so that the LLM can generate a response \nbased on the provided context.   \n \nFinally, the \"G\" or Generation is the phase during which the generator LLM processes \nthe prompt having instructions, a query, and context to generate a response that is \ncoherent, accurate, and contextually relevant.  \n \nQ6. How does RAG help reduce hallucinations in LLM generated responses? \n \nWithout RAG, LLM answers user questions based on what it learned from the training \ncorpu",
    "chunk_index": 5
  },
  {
    "chunk_id": "doc4_6",
    "file_name": "doc4.pdf",
    "text": "AG help reduce hallucinations in LLM generated responses? \n \nWithout RAG, LLM answers user questions based on what it learned from the training \ncorpus, which may not be up-to-date or complete. This could lead to hallucinated \nresponses, which are answers that sound right but are wrong. \n \nRetrieval-Augmented Generation (RAG) helps cut down on hallucinations in \nLLM-generated responses by adding an external retrieval system that pulls relevant, \nfactual information from trusted, up-to-date external knowledge sources. \n \n2                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nBy combining retrieval with generation, RA",
    "chunk_index": 6
  },
  {
    "chunk_id": "doc4_7",
    "file_name": "doc4.pdf",
    "text": "r                                                          aixfunda.substack.com                         \n \nBy combining retrieval with generation, RAG ensures that answers are more accurate, \ncontextually relevant, and less prone to fabrications or false information, significantly \nenhancing the reliability of the output.  \n \nQ7. Why is re-ranking important in the RAG pipeline after initial document \nretrieval? \n \nThe top K chunks fetched by the RAG retriever may have irrelevant chunks ahead of \nrelevant ones. Passing these results directly to the LLM hurts the quality of the answers \nbecause LLMs mostly look at the top-ranked chunks that are given as context.  \n \nRe-ranking uses cross-encoder models to deeply measure the semantic relevance of  \nquery-chunk pairs and then brings relevant ",
    "chunk_index": 7
  },
  {
    "chunk_id": "doc4_8",
    "file_name": "doc4.pdf",
    "text": "e given as context.  \n \nRe-ranking uses cross-encoder models to deeply measure the semantic relevance of  \nquery-chunk pairs and then brings relevant chunks ahead of irrelevant chunks. This \nreduces the noise and helps the generator LLM to generate more accurate and coherent \nanswers.  \n \nQ8. What is the purpose of character overlap during chunking in a RAG \npipeline? \n \nIn a RAG pipeline, chunk overlap during chunking ensures contextual continuity and \nprevents loss of information at the boundaries of chunks. This improves the retrieval \naccuracy and maintains coherence in the text fed to the LLM.  \n \nTypically, an overlap of about 10-20% of the chunk size is used to strike a balance \nbetween preserving context and computational efficiency in RAG applications. \n \nQ9. What role does cosine",
    "chunk_index": 8
  },
  {
    "chunk_id": "doc4_9",
    "file_name": "doc4.pdf",
    "text": " the chunk size is used to strike a balance \nbetween preserving context and computational efficiency in RAG applications. \n \nQ9. What role does cosine similarity play in relevant chunk retrieval within \na RAG pipeline? \n \nCosine similarity measures how similar the query embedding is to the embeddings of \nchunks in the vector database. It finds the cosine of the angle between two vectors and \nprovides a score that shows how closely related the query is to each chunk. Higher \nscores mean that the chunk is more relevant.  \n \nThis enables the RAG system to retrieve the most relevant chunks for the query, which is \nthen used by the generator LLM to generate accurate answers. \n \nQ10. Can you give examples of real-world applications where RAG systems \nhave demonstrated value? \n3                  ",
    "chunk_index": 9
  },
  {
    "chunk_id": "doc4_10",
    "file_name": "doc4.pdf",
    "text": "o generate accurate answers. \n \nQ10. Can you give examples of real-world applications where RAG systems \nhave demonstrated value? \n3                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nAI search engines are a great example of how RAG systems have changed the way people \nfind information online. AI search engines give you accurate, relevant answers by \ncombining information retrieval with generative AI.  \n \nFor instance, RAG-based AI search platforms like Perplexity AI improve the user \nexperience by fetching the most recent and relevant information from large knowledge \nbases and then giving it back in the format",
    "chunk_index": 10
  },
  {
    "chunk_id": "doc4_11",
    "file_name": "doc4.pdf",
    "text": "AI improve the user \nexperience by fetching the most recent and relevant information from large knowledge \nbases and then giving it back in the format that the user wants. \n \nQ11. Explain the steps in the indexing process in a RAG pipeline. \n \nThere are four steps in the indexing process of a RAG pipeline: parsing, chunking, \nencoding, and storing. The parsing step deals with extracting the document content. \nThen, the chunking step splits the extracted content into smaller pieces called chunks.  \n \nThe encoding step uses an embedding model to convert chunks into dense numerical \nvectors called embeddings. Finally, these embeddings are saved in a vector database for \nefficient search and retrieval. \n \nAll these steps in the indexing process are performed offline. \n \nQ12. Explain the import",
    "chunk_index": 11
  },
  {
    "chunk_id": "doc4_12",
    "file_name": "doc4.pdf",
    "text": "in a vector database for \nefficient search and retrieval. \n \nAll these steps in the indexing process are performed offline. \n \nQ12. Explain the importance of chunking in RAG. \n \nChunking in Retrieval-Augmented Generation (RAG) is crucial because it breaks down \nlarge texts into smaller and semantically coherent segments called chunks. Proper \nchunking helps to find relevant information efficiently by creating focused chunks that \nmaintain context and avoid irrelevant noise.  \n \nChoosing the right chunk size balances detail and context, optimizing both retrieval \naccuracy and computational efficiency. Ineffective chunking can lead to poor retrieval \nresults and incoherent responses, which makes it a foundational step for successful RAG \nperformance in real-world applications. \n \nQ13. How do",
    "chunk_index": 12
  },
  {
    "chunk_id": "doc4_13",
    "file_name": "doc4.pdf",
    "text": "ieval \nresults and incoherent responses, which makes it a foundational step for successful RAG \nperformance in real-world applications. \n \nQ13. How do you choose the chunk size for a RAG system? \n \nChoosing the chunk size for a RAG system involves balancing granularity, context \ncompleteness, and computational efficiency. Smaller chunks (e.g., 100-200 tokens) \nallow precise retrieval but may lack sufficient context. Larger chunks (e.g., 500-1000 \ntokens) provide more context at the cost of increased computational load and potential \nnoise.  \n4                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nThe optimal size d",
    "chunk_index": 13
  },
  {
    "chunk_id": "doc4_14",
    "file_name": "doc4.pdf",
    "text": "\n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nThe optimal size depends on the use case, document structure, embedding model, and \nthe generator (LLM) model. For example, smaller chunks are suitable for fact-based \nqueries, and more complex queries benefit from larger ones. \n \nQ14. What are the potential consequences of having chunks that are too \nlarge versus chunks that are too small? \n \nLarge chunks often mix different topics into one chunk and reduce the chunk's \nrelevance. This can lead to coarse vector representations and less accurate retrieval. \nLarge chunks can also add noise and confuse the model with irrelevant information that \nisn't important, resulting in a less accurate answer. \n \nSmall chun",
    "chunk_index": 14
  },
  {
    "chunk_id": "doc4_15",
    "file_name": "doc4.pdf",
    "text": " chunks can also add noise and confuse the model with irrelevant information that \nisn't important, resulting in a less accurate answer. \n \nSmall chunk sizes in RAG systems can lead to fragmented context. This fragmentation \noften leads to poor retrieval quality because information that is semantically related may \nbe split up into chunks that are not retrieved together. Furthermore, smaller chunks \nmean that there are more chunks in the vector database, which increases storage costs \nand slows down the similarity search. \n \nQ15. Explain the retrieval process step-by-step in a RAG pipeline. \n \nThe retrieval process in RAG systems starts by encoding the user query, i.e., converting \nit into a dense vector representation using an embedding model.  \nThis vector \nrepresentation is then used to",
    "chunk_index": 15
  },
  {
    "chunk_id": "doc4_16",
    "file_name": "doc4.pdf",
    "text": "coding the user query, i.e., converting \nit into a dense vector representation using an embedding model.  \nThis vector \nrepresentation is then used to search the vector database, which has the embeddings of \nchunks.  Based on the similarity scores, the vector database system returns the most \nrelevant document chunks.  \n \nQ16. What are the key considerations when choosing an LLM for a RAG \nsystem? \n \nThe \nkey \nconsiderations \nwhen \nchoosing \nan \nLLM \nfor \na RAG system are \nreading-comprehension \nability, \ncontext \nwindow \nsize, \nand \ninference \nspeed. \nReading-comprehension ability reflects how effectively the model processes the \nretrieved context to generate accurate responses.  \n \nContext window size is crucial, as longer context models enable RAG systems to \neffectively include more re",
    "chunk_index": 16
  },
  {
    "chunk_id": "doc4_17",
    "file_name": "doc4.pdf",
    "text": "ontext to generate accurate responses.  \n \nContext window size is crucial, as longer context models enable RAG systems to \neffectively include more relevant chunks. However, this must be balanced against cost \nand latency requirements.  \n \n5                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nAdditionally, inference speed, infrastructure compatibility, and licensing terms also play \na key role in deployment decisions for real-world RAG solutions. \n \nQ17. How is the prompt provided to the LLM in a RAG system different from \na standard, non-RAG prompt? \n \nThe prompt provided to the LLM without a RAG setup includes on",
    "chunk_index": 17
  },
  {
    "chunk_id": "doc4_18",
    "file_name": "doc4.pdf",
    "text": "ompt provided to the LLM in a RAG system different from \na standard, non-RAG prompt? \n \nThe prompt provided to the LLM without a RAG setup includes only the user query and \nthe optional instructions. Here, the LLM generates the response based on its knowledge \ngained during training. \n \nThe prompt provided to the LLM with the RAG setup includes the user query, \ninstructions, and relevant context. Here, the LLM generates the response as per the \ninstructions solely based on the provided relevant context.  \n \nQ18. What are the key hyperparameters in a RAG pipeline? \n \nChunk size, chunk overlap, embedding dimensionality, retrieval top-k, and retrieval \nthreshold are some of the most important hyperparameters for retrieval in RAG. \nTemperature and max output length are two important hyperparam",
    "chunk_index": 18
  },
  {
    "chunk_id": "doc4_19",
    "file_name": "doc4.pdf",
    "text": "rieval \nthreshold are some of the most important hyperparameters for retrieval in RAG. \nTemperature and max output length are two important hyperparameters for RAG \ngeneration. \n \nThe chunk size determines how much text is put into a segment before embedding, \ninfluencing the context granularity retrieved. Chunk overlap repeats a set of tokens at \nchunk boundaries, helping preserve important context across segments. Embedding \ndimensionality is the vector size used to represent text, which affects retrieval precision \nand database efficiency. \n \nRetrieval top-k sets the number of most similar chunks returned, directly impacting \nrecall and context diversity in the response. The retrieval threshold is a similarity cutoff \nthat filters retrieved results, ensuring only relevant chunks are sel",
    "chunk_index": 19
  },
  {
    "chunk_id": "doc4_20",
    "file_name": "doc4.pdf",
    "text": "ntext diversity in the response. The retrieval threshold is a similarity cutoff \nthat filters retrieved results, ensuring only relevant chunks are selected. \n \nTemperature controls the randomness of generated text, balancing creativity and \ndeterminism in model outputs. Max output length limits the number of tokens \ngenerated, managing the verbosity and computational cost of responses. \n \nQ19. What are the popular frameworks to implement a RAG system? Justify \nyour choice of framework. \n \nLangChain, LlamaIndex, and Haystack are the most popular frameworks for RAG \nimplementation. LangChain is great for custom pipelines, and LlamaIndex is great for \n6                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                  ",
    "chunk_index": 20
  },
  {
    "chunk_id": "doc4_21",
    "file_name": "doc4.pdf",
    "text": "t for \n6                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nefficient document indexing and retrieval. The Haystack framework provides excellent \nmodularity for building RAG systems. \n \nI would recommend LangChain because of its comprehensive ecosystem, extensive \ndocumentation, active community support, and flexibility in handling various data \nsources and LLM integrations. \n \nQ20. Explain the influence of LLM context window size on RAG \nhyperparameters. \n \nThe size of the LLM context window has a big impact on RAG hyperparameters, like \nchunk size and the number of chunks that are retrieved. Larger context windo",
    "chunk_index": 21
  },
  {
    "chunk_id": "doc4_22",
    "file_name": "doc4.pdf",
    "text": " of the LLM context window has a big impact on RAG hyperparameters, like \nchunk size and the number of chunks that are retrieved. Larger context windows let you \nfeed more retrieved chunks to the LLM, which increases the chance of including more \nrelevant information. This could make the quality of the generated answers better.  \n \nBut after a certain point, performance gains start to go down because of problems like \n\"lost in the middle\" and higher latency. \n \nQ21. How do you choose values for various LLM inference hyperparameters \nin a RAG system? \n \nTemperature controls randomness—lower values give more focused and deterministic \nresponses suitable for technical or precise tasks, while higher values make output more \ncreative and diverse.  \n \nThe max tokens limit the length of the outpu",
    "chunk_index": 22
  },
  {
    "chunk_id": "doc4_23",
    "file_name": "doc4.pdf",
    "text": " suitable for technical or precise tasks, while higher values make output more \ncreative and diverse.  \n \nThe max tokens limit the length of the output, making sure that the answers are short or \nlong enough depending on the use case, with a trade-off between completeness and \nlatency. Optimal settings depend on the specific application context and are found \nthrough iterative experimentation.  \n \nQ22. Compare reasoning vs. non-reasoning LLMs for RAG systems. \n \nReasoning LLMs such as GPT-4o1 and DeepSeek R1 are better generators in RAG \nsystems because they have advanced \"test-time compute\" and chain-of-thought features. \nThese unique abilities allow them to analyze the retrieved information more effectively \nand do multi-step reasoning to come up with better answers.   \n \n7              ",
    "chunk_index": 23
  },
  {
    "chunk_id": "doc4_24",
    "file_name": "doc4.pdf",
    "text": "ies allow them to analyze the retrieved information more effectively \nand do multi-step reasoning to come up with better answers.   \n \n7                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nBut non-reasoning LLMs are still cheaper and faster, which makes them a good choice \nfor many applications. In the end, the choice between reasoning and non-reasoning \nmodels depends on the query complexity.   \n \nQ23. What happens with a weak generator LLM in a RAG system? \n \nA weak generator LLM may find it difficult to understand the retrieved context, which \ncould lead to answers that are incomplete or hallucinated. This makes",
    "chunk_index": 24
  },
  {
    "chunk_id": "doc4_25",
    "file_name": "doc4.pdf",
    "text": " generator LLM may find it difficult to understand the retrieved context, which \ncould lead to answers that are incomplete or hallucinated. This makes the whole RAG \nsystem less useful because the final answers lack coherence and factual correctness, \neven though the context that was retrieved is good. \n \nSo, in a RAG setup, a strong generator LLM is necessary to convert retrieved knowledge \ninto reliable, contextually relevant outputs. \n \n \n \n \n                  ☕ Support the Author \n \nI hope you found this “RAG Interview Questions and Answers” book highly \nuseful.   \n \nI’ve made this book freely available to help the AI and NLP community grow \nand to support learners like you. If you found it helpful and would like to \nshow your appreciation, you can buy me a coffee to keep me motivated ",
    "chunk_index": 25
  },
  {
    "chunk_id": "doc4_26",
    "file_name": "doc4.pdf",
    "text": "ow \nand to support learners like you. If you found it helpful and would like to \nshow your appreciation, you can buy me a coffee to keep me motivated in \ncreating more free resources like this. \n \n👉 Buy Me a Coffee \n \nYour small gesture goes a long way in supporting my work—thank you for \nbeing part of this journey! 🙏 \n \n— Kalyan KS \n \n \n \n \n8                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ24. How do you handle ambiguous or vague user queries in RAG systems? \n \nIssues with ambiguous or vague user queries in RAG systems include retrieval of \nirrelevant information, incomplete answers, and increased risk of hal",
    "chunk_index": 26
  },
  {
    "chunk_id": "doc4_27",
    "file_name": "doc4.pdf",
    "text": "Issues with ambiguous or vague user queries in RAG systems include retrieval of \nirrelevant information, incomplete answers, and increased risk of hallucination due to \nthe lack of specificity.  \n \nThe most common strategy to handle ambiguous or vague user queries is query \nrewriting.  Query rewriting transforms unclear queries into precise and focused queries, \nthereby enhancing retrieval quality and leading to more accurate, grounded responses. \n \nQ25. What are the different query transformation techniques that enhance \nuser queries in RAG? \n \nDifferent query transformation techniques in RAG include query rewriting, query \nexpansion, query decomposition, and HyDE to enhance retrieval relevance and context \nprecision.  \n \nQuery Rewriting: Rewrites the initial user query to make it more sp",
    "chunk_index": 27
  },
  {
    "chunk_id": "doc4_28",
    "file_name": "doc4.pdf",
    "text": "decomposition, and HyDE to enhance retrieval relevance and context \nprecision.  \n \nQuery Rewriting: Rewrites the initial user query to make it more specific and detailed, \nboosting retrieval accuracy. \n \nQuery Expansion using Step-Back Prompting: Generates a broader, generalized version \nof the query. \n \nQuery Decomposition: Divides complex queries into simpler sub-queries to ensure \ncomprehensive coverage and more precise retrieval for each component question. \n \nHyDE (Hypothetical Document Embedding): Synthesizes a hypothetical answer to the \nquery and uses it as a retrieval query to get more relevant document chunks. \n \nQ26. What are the pros and cons of query transformation techniques? \n \nQuery transformation techniques in RAG systems offer significant advantages, such as \nimproved ret",
    "chunk_index": 28
  },
  {
    "chunk_id": "doc4_29",
    "file_name": "doc4.pdf",
    "text": "ros and cons of query transformation techniques? \n \nQuery transformation techniques in RAG systems offer significant advantages, such as \nimproved retrieval accuracy leading to more relevant and contextually accurate \nresponses.  \n \nHowever, their downsides include increased computational cost, added latency, and \npotential noise from overexpansion. Over expansion risks retrieving noisy or off-topic \ndocuments, while complex methods like query decomposition require careful handling \nto ensure subqueries align with the original intent.  \n \n9                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nSome strategies may als",
    "chunk_index": 29
  },
  {
    "chunk_id": "doc4_30",
    "file_name": "doc4.pdf",
    "text": "🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nSome strategies may also require substantial prompt engineering and continuous \noptimization to match diverse query scenarios. Balancing effectiveness and efficiency is \ncritical to avoid diminishing returns. \n \nQ27. Explain how the HyDE query transformation technique works. \n \nThe HyDE (Hypothetical Document Embedding) technique improves RAG retrieval by \ntransforming the user query into a hypothetical answer before embedding it. Rather \nthan directly searching with the query embedding, the HyDE technique utilizes a large \nlanguage model (LLM) to create a brief, plausible document that could potentially \nanswer the query.  \n \nThis synthetic document is then encode",
    "chunk_index": 30
  },
  {
    "chunk_id": "doc4_31",
    "file_name": "doc4.pdf",
    "text": "large \nlanguage model (LLM) to create a brief, plausible document that could potentially \nanswer the query.  \n \nThis synthetic document is then encoded into an embedding and used for retrieval, \nleading to better semantic alignment with actual document chunks in the database. As a \nresult, HyDE enhances retrieval quality, especially for vague or underspecified queries. \n \nQ28. Explain how the HyPE technique works in RAG. \n \nThe HyPE (Hypothetical Prompt Embedding) technique improves retrieval accuracy by \naddressing the semantic mismatch between user queries and document chunks.  \n \nUnlike HyDE, which generates hypothetical answer documents at query time, HyPE \nprecomputes hypothetical questions for each document chunk during the indexing \nphase. These questions are designed to capture the",
    "chunk_index": 31
  },
  {
    "chunk_id": "doc4_32",
    "file_name": "doc4.pdf",
    "text": "t query time, HyPE \nprecomputes hypothetical questions for each document chunk during the indexing \nphase. These questions are designed to capture the key concepts in the chunk, \ntransforming retrieval into a \"question-to-question\" matching process, which reduces \nlatency and improves retrieval. \n \nQ29. Compare HyPE and HyDE techniques in RAG. \n \nHyDE (Hypothetical Document Embedding) and HyPE (Hypothetical Prompt \nEmbedding) enhance RAG by addressing the semantic gap between user queries and \ndocument chunks, but they differ in approach. \n \nTiming: HyPE generates hypothetical questions during indexing, while HyDE generates \nhypothetical answer documents at query time. \n \nEfficiency: HyPE reduces runtime latency by avoiding LLM calls during retrieval, unlike \nHyDE, which requires an LLM ca",
    "chunk_index": 32
  },
  {
    "chunk_id": "doc4_33",
    "file_name": "doc4.pdf",
    "text": "er documents at query time. \n \nEfficiency: HyPE reduces runtime latency by avoiding LLM calls during retrieval, unlike \nHyDE, which requires an LLM call per query. \n \n10                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nFocus: HyPE focuses on question-question matching, while HyDE focuses on \nanswer-answer matching. \n \nWhile HyDE is flexible for diverse queries, HyPE’s pre-indexed approach is more \nefficient for real-time applications. \n \nQ30. To minimize RAG system latency, which pre-retrieval enhancement \ntechnique will you choose? \n \nTo minimize RAG system latency, I would choose the HyPE (Hypothetical Prompt ",
    "chunk_index": 33
  },
  {
    "chunk_id": "doc4_34",
    "file_name": "doc4.pdf",
    "text": " latency, which pre-retrieval enhancement \ntechnique will you choose? \n \nTo minimize RAG system latency, I would choose the HyPE (Hypothetical Prompt \nEmbedding) technique. Unlike query transformation techniques such as query \nrewriting, query expansion, query decomposition, or HyDE, which require LLM calls at \nquery time and increase latency, HyPE precomputes hypothetical questions for \ndocument chunks during the indexing phase.  \n \nThis question-to-question matching approach reduces runtime latency by avoiding \nreal-time LLM calls, making it more efficient for real-time applications while \nmaintaining high retrieval accuracy. By shifting the computational effort to indexing, \nHyPE ensures faster and more precise document retrieval. \n \nQ31. What are the different chunk enhancement techniq",
    "chunk_index": 34
  },
  {
    "chunk_id": "doc4_35",
    "file_name": "doc4.pdf",
    "text": "e computational effort to indexing, \nHyPE ensures faster and more precise document retrieval. \n \nQ31. What are the different chunk enhancement techniques in RAG? \n \nThe different chunk enhancement techniques in RAG are HyPE, Contextual Chunk \nHeader, and Document Augmentation. \n \nHyPE (Hypothetical Prompt Embedding) precomputes hypothetical questions for each \ndocument chunk at indexing time, enabling retrieval by question-to-question matching, \nwhich improves semantic alignment and retrieval accuracy without adding query-time \nlatency. \n \nContextual Chunk Header adds relevant contextual information such as document titles \nor section headings to each chunk before embedding, helping retrieval models \nunderstand and rank chunks better when chunk text alone is ambiguous. \n \nDocument Augmenta",
    "chunk_index": 35
  },
  {
    "chunk_id": "doc4_36",
    "file_name": "doc4.pdf",
    "text": "s to each chunk before embedding, helping retrieval models \nunderstand and rank chunks better when chunk text alone is ambiguous. \n \nDocument Augmentation enhances chunks by including additional metadata and \nenhances retrieval quality. \n \n \n11                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ32. What are the pros and cons of chunk enhancement techniques in \nRAG? \n \nChunk enhancement techniques in RAG, such as HyPE, Contextual Chunk Header, and \nDocument Augmentation, improve retrieval accuracy by enhancing semantic alignment, \npreserving context, and bridging query-document chunk gaps, leading to better \ngener",
    "chunk_index": 36
  },
  {
    "chunk_id": "doc4_37",
    "file_name": "doc4.pdf",
    "text": "ion, improve retrieval accuracy by enhancing semantic alignment, \npreserving context, and bridging query-document chunk gaps, leading to better \ngeneration performance.  \n \nHyPE boosts relevance through precomputed question embeddings without query-time \nlatency, Contextual Chunk Header clarifies ambiguous chunks with document or section \ntitles, and Document Augmentation enriches chunks with additional metadata.  \n \nHowever, these methods increase indexing complexity and storage requirements, \npotentially raising computational costs. Balancing enhanced retrieval quality with \nresource demands is a key consideration. \n \nQ33. Explain how the contextual chunk header technique enhances RAG \nretrieval. \n \nThe Contextual Chunk Header technique in RAG enhances retrieval by adding  \ndocument titl",
    "chunk_index": 37
  },
  {
    "chunk_id": "doc4_38",
    "file_name": "doc4.pdf",
    "text": "ontextual chunk header technique enhances RAG \nretrieval. \n \nThe Contextual Chunk Header technique in RAG enhances retrieval by adding  \ndocument titles, section headings, or summaries to each chunk before embedding, \nproviding critical context that clarifies ambiguous or isolated chunk content.  This \nadditional information helps the retrieval model better understand the chunk’s \nrelevance to a query, improving semantic alignment and ranking accuracy.  \n \nQ34. What are some common chunking methods used in RAG? \n \nCommon chunking methods used in RAG are fixed-size chunking, recursive chunking, \nsemantic chunking, and agentic chunking.  \n \nFixed-size chunking divides text into uniform segments based on a predefined token or \ncharacter length, often incorporating overlap to maintain context.",
    "chunk_index": 38
  },
  {
    "chunk_id": "doc4_39",
    "file_name": "doc4.pdf",
    "text": "ed-size chunking divides text into uniform segments based on a predefined token or \ncharacter length, often incorporating overlap to maintain context.  \n \nRecursive chunking iteratively splits text using natural separators like paragraphs or \nsentences to preserve logical boundaries. Semantic chunking groups text based on \nsemantic similarity using embeddings, creating coherent, meaning-based chunks.  \n \nAgentic chunking leverages AI agents to dynamically segment text into task-oriented, \nsemantically coherent chunks, often with metadata to enhance retrieval relevance. \n \n12                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                ",
    "chunk_index": 39
  },
  {
    "chunk_id": "doc4_40",
    "file_name": "doc4.pdf",
    "text": "Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ35. What are the criteria to choose a specific chunking method in RAG? \n \nThe criteria for choosing a specific chunking method in RAG include the nature and \nstructure of the source documents, capabilities of the embedding model, and the specific \ntask or application needs.   \n \nFor structured or well-formatted data, semantic or agentic chunking ensures logical \nboundaries and context preservation. The chunk size must balance between being large \nenough to capture meaningful context and small enough to fit within model constraints \nfor efficient processing.  \n \nTask specificity matters since complex tasks may require semantic or ",
    "chunk_index": 40
  },
  {
    "chunk_id": "doc4_41",
    "file_name": "doc4.pdf",
    "text": " and small enough to fit within model constraints \nfor efficient processing.  \n \nTask specificity matters since complex tasks may require semantic or agentic chunking \nfor better context and relevance, while simpler cases can use fixed-size chunking.  \n \nUltimately, the chunking method should balance retrieval relevance, context \ncompleteness, and computational efficiency. \n \nQ36. Explain the pros and cons of semantic chunking. \n \nSemantic chunking groups text based on meaning, creating coherent chunks that \nenhance retrieval relevance and context preservation.  \n \nPros: It aligns chunks with natural topic shifts, improving the quality of retrieved \ncontent for complex queries, and reduces information loss across boundaries.  \n \nCons: It is computationally intensive, requiring embedding mo",
    "chunk_index": 41
  },
  {
    "chunk_id": "doc4_42",
    "file_name": "doc4.pdf",
    "text": "rieved \ncontent for complex queries, and reduces information loss across boundaries.  \n \nCons: It is computationally intensive, requiring embedding models. Additionally, it may \nstruggle with highly complex or poorly structured documents where semantic \nboundaries are unclear. \n \n \nQ37. How does the chunking strategy differ when dealing with structured \ndocuments (like PDFs with tables and figures) versus plain text documents? \n \nChunking strategies for structured documents like PDFs with tables and figures differ \nsignificantly from plain text chunking due to the need to preserve complex layouts and \nrelationships.  For structured documents, the chunking strategy must respect document \nelements such as tables, figures, headers, and pages to maintain context and semantic \nmeaning.  \n \n13  ",
    "chunk_index": 42
  },
  {
    "chunk_id": "doc4_43",
    "file_name": "doc4.pdf",
    "text": " the chunking strategy must respect document \nelements such as tables, figures, headers, and pages to maintain context and semantic \nmeaning.  \n \n13                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nAgentic and recursive chunking are more suitable for structured documents due to their \nflexibility in respecting structure and context. Fixed-size and semantic chunking are \noften better suited for plain text documents where semantic coherence and simplicity \nare prioritized. \n \n \n \n                                                        LLM Engineer Toolkit \n 🤖 This repository contains a curated list of 120+ LLM lib",
    "chunk_index": 43
  },
  {
    "chunk_id": "doc4_44",
    "file_name": "doc4.pdf",
    "text": "tized. \n \n \n \n                                                        LLM Engineer Toolkit \n 🤖 This repository contains a curated list of 120+ LLM libraries category \nwise.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis repository is highly useful for AI/ML Engineers.  \n \n \n \n \n \n \n \n \n \n \n \n \n14                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ38. What are the possible reasons for the poor performance of a RAG \nretriever? \n \nThe possible reasons for the poor performance of a RAG retriever are an outdated or \nincomplete knowledge base, a weak retrieval model, low-quality embeddings, and lack \nof domain-specific fine-tuni",
    "chunk_index": 44
  },
  {
    "chunk_id": "doc4_45",
    "file_name": "doc4.pdf",
    "text": " a RAG retriever are an outdated or \nincomplete knowledge base, a weak retrieval model, low-quality embeddings, and lack \nof domain-specific fine-tuning. \n \nAn outdated or incomplete knowledge base prevents the retriever from accessing recent \nor relevant information, limiting answer accuracy. A weak retrieval model, such as using \nTF-IDF or BM25 instead of dense vector models, leads to less effective retrieval of \nrelevant context. \n \nLow-quality embeddings reduce the semantic understanding between queries and \ndocument chunks, causing mismatches. Lack of domain-specific fine-tuning results in \nretrieval errors because the embedding model doesn’t fully capture the nuances or \nterminology of the target domain. \n \nQ39. What happens with a weak retriever in Retrieval-Augmented \nGeneration (R",
    "chunk_index": 45
  },
  {
    "chunk_id": "doc4_46",
    "file_name": "doc4.pdf",
    "text": "esn’t fully capture the nuances or \nterminology of the target domain. \n \nQ39. What happens with a weak retriever in Retrieval-Augmented \nGeneration (RAG) systems? \n \nA weak retriever in RAG systems leads to the retrieval of irrelevant or noisy document \nchunks. This can significantly degrade the quality of generated answers, as the RAG \ngenerator relies heavily on the retrieved context. The presence of irrelevant or noisy \ndocument chunks in the context because of poor retrieval causes the generator model to \nproduce answers that are inaccurate or hallucinated while still appearing fluent.  \n \nTherefore, strong retrievers are necessary to provide the most relevant context and \nensure factual and relevant outputs in RAG systems. \n \nQ40. What are the common retrieval approaches used in RAG s",
    "chunk_index": 46
  },
  {
    "chunk_id": "doc4_47",
    "file_name": "doc4.pdf",
    "text": "vide the most relevant context and \nensure factual and relevant outputs in RAG systems. \n \nQ40. What are the common retrieval approaches used in RAG systems? \n \nCommon retrieval approaches in RAG systems include dense retrieval, sparse retrieval, \nand hybrid retrieval. Dense retrieval uses embeddings to capture semantic similarity, \nenabling effective query matching to relevant document chunks. Sparse retrieval relies \non traditional methods like TF-IDF or BM25, focusing on keyword-based matching for \nefficiency.  \n \nHybrid retrieval combines dense and sparse methods to balance semantic \nunderstanding and computational speed. These approaches ensure relevant context is \nretrieved for generating accurate responses in RAG systems. \n15                                                          ",
    "chunk_index": 47
  },
  {
    "chunk_id": "doc4_48",
    "file_name": "doc4.pdf",
    "text": " ensure relevant context is \nretrieved for generating accurate responses in RAG systems. \n15                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nQ41. What are some common challenges in RAG retrieval? \n \nCommon challenges in RAG retrieval include ineffective query understanding, \nscalability issues, context fragmentation, and handling multimodal data.  Ineffective \nquery understanding leads to misinterpreting user intent, resulting in irrelevant \nretrieved document chunks.   \n \nScalability issues arise when large-scale data retrieval slows performance or overwhelms \nthe system.   Context fragmentation happens when",
    "chunk_index": 48
  },
  {
    "chunk_id": "doc4_49",
    "file_name": "doc4.pdf",
    "text": "hunks.   \n \nScalability issues arise when large-scale data retrieval slows performance or overwhelms \nthe system.   Context fragmentation happens when retrieved chunks lack sufficient \ncontext, lowering response quality.   Handling multimodal data is challenging due to \ncomplexities in integrating text, images, or other formats effectively. \n \nQ42. What are the key metrics for evaluating retrieval quality in RAG? \n \nKey metrics for evaluating retrieval quality in RAG systems are precision, recall, Mean \nReciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). \nPrecision measures the proportion of retrieved document chunks that are relevant, \nwhile recall assesses the proportion of relevant document chunks retrieved from the \ntotal available.  \n \nMRR evaluates the ranking qu",
    "chunk_index": 49
  },
  {
    "chunk_id": "doc4_50",
    "file_name": "doc4.pdf",
    "text": " are relevant, \nwhile recall assesses the proportion of relevant document chunks retrieved from the \ntotal available.  \n \nMRR evaluates the ranking quality by considering the position of the first relevant \ndocument chunks, and NDCG accounts for the relevance and ranking of retrieved \ndocuments. These metrics collectively ensure the retriever effectively identifies and \nranks relevant information. \n \nQ43. What are embeddings, and how are they utilized in RAG retrieval? \n \nEmbeddings are numerical vector representations of text that capture the semantic \nmeaning \nand \nrelationships \nof \nthe \ndata \nin \na \nhigh-dimensional \nspace. \nIn \nRetrieval-Augmented Generation (RAG), embeddings are used to convert both the user \nquery and document chunks into vectors, enabling semantic search by compari",
    "chunk_index": 50
  },
  {
    "chunk_id": "doc4_51",
    "file_name": "doc4.pdf",
    "text": "-Augmented Generation (RAG), embeddings are used to convert both the user \nquery and document chunks into vectors, enabling semantic search by comparing these \nvectors for similarity.  \n \nThis process allows RAG systems to retrieve the most relevant and contextually \nappropriate document chunks from a knowledge base, which are then used as context to \ngenerate accurate and grounded responses. Thus, embeddings form the backbone of \nRAG retrieval by enabling efficient, meaning-driven retrieval beyond simple keyword \nmatching. \n \n16                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ44. What are the key consideratio",
    "chunk_index": 51
  },
  {
    "chunk_id": "doc4_52",
    "file_name": "doc4.pdf",
    "text": "ewsletter                                                          aixfunda.substack.com                         \n \nQ44. What are the key considerations when choosing an embedding model \nfor a RAG system? \n \nWhen choosing an embedding model for a RAG system, key considerations include  \n \n(i) the model's domain relevance to ensure it accurately captures domain-specific \nsemantics,  \n(ii) embedding dimensionality, which balances retrieval precision against computational \nand storage costs, and  \n(ii) embedding model performance on the specific dataset to ensure good retrieval \nquality. This is necessary, as the real-world data often differ from academic datasets.  \n(iv) Additionally, factors such as embedding model size, API availability, latency, cost \nimplications, and licensing should be",
    "chunk_index": 52
  },
  {
    "chunk_id": "doc4_53",
    "file_name": "doc4.pdf",
    "text": "m academic datasets.  \n(iv) Additionally, factors such as embedding model size, API availability, latency, cost \nimplications, and licensing should be considered to align with infrastructure constraints \nand use case requirements.  \n \nChoosing the right embedding model directly impacts the effectiveness and scalability of \nthe RAG system. \n \nQ45. What is a VectorDB, and how is it utilized in RAG retrieval? \n \nA vector database, or VectorDB for short, is a specialized database designed to store and \nretrieve high-dimensional vector embeddings. In RAG retrieval, VectorDB is utilized to \nefficiently perform semantic searches by matching the vector representation of a user \nquery with the closest vectors stored in the database, thereby retrieving the most \ncontextually relevant document chunks",
    "chunk_index": 53
  },
  {
    "chunk_id": "doc4_54",
    "file_name": "doc4.pdf",
    "text": "or representation of a user \nquery with the closest vectors stored in the database, thereby retrieving the most \ncontextually relevant document chunks.  \n \nVectorDBs enable scalable and fast similarity search, which is crucial for the RAG \nsystems. \n \nQ46. Explain the role of ANN (Approximate Nearest Neighbor) search \nalgorithms in RAG retrieval. \n \nApproximate Nearest Neighbor (ANN) search algorithms play a crucial role in RAG \nretrieval by enabling fast and scalable search of relevant document chunks within large \nvector databases.  Approximate Nearest Neighbor (ANN) algorithms enable fast search \nin RAG retrieval by quickly narrowing down the search space to a small subset of \ncandidate vectors instead of scanning all vectors.  \n \n17                                                      ",
    "chunk_index": 54
  },
  {
    "chunk_id": "doc4_55",
    "file_name": "doc4.pdf",
    "text": "the search space to a small subset of \ncandidate vectors instead of scanning all vectors.  \n \n17                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nThis reduces the number of comparisons needed, significantly speeding up retrieval \nwhile maintaining good enough accuracy for relevant document chunks matching. This \nbalance of speed and precision is crucial for real-time and large-scale RAG systems. \n \nQ47. Explain the step-by-step working of ANN algorithms for fast search in \nRAG retrieval. \n \nANN algorithms for fast search in RAG retrieval involve four steps namely - Encoding, \nIndexing, Navigating, Retrieving.  \n",
    "chunk_index": 55
  },
  {
    "chunk_id": "doc4_56",
    "file_name": "doc4.pdf",
    "text": "arch in \nRAG retrieval. \n \nANN algorithms for fast search in RAG retrieval involve four steps namely - Encoding, \nIndexing, Navigating, Retrieving.  \n \n(i) Encoding: Convert document chunks and queries into vector representations.  \n(ii) Indexing: Organize these vectors into a specialized data structure (like graphs or \nhash tables) for quick lookup. \n(iii) Navigating: Efficiently explore the index to find vectors close to the query without \nchecking all data points.  \n(iv) Retrieving: Return the closest approximate neighbors that provide relevant \ninformation for RAG retrieval.  \n \nThis approach balances search speed and accuracy, enabling fast retrieval in large-scale \nRAG systems.  \n \nQ48. What are the typical distance metrics used for similarity search in \nvector databases, and why are",
    "chunk_index": 56
  },
  {
    "chunk_id": "doc4_57",
    "file_name": "doc4.pdf",
    "text": "ast retrieval in large-scale \nRAG systems.  \n \nQ48. What are the typical distance metrics used for similarity search in \nvector databases, and why are they chosen? \n \nTypical distance metrics used in vector databases for similarity search are Euclidean \ndistance, cosine similarity, and dot product similarity. Euclidean distance measures the \nstraight-line distance between vectors, making it intuitive for geometric closeness. \nCosine similarity evaluates the angle between vectors, focusing on their direction \n(meaning) rather than magnitude.  \n \nDot product similarity considers both magnitude and direction. These metrics are \nselected based on the data type and the underlying embedding model to ensure effective \nand accurate retrieval. \n \nQ49. Explain why cosine similarity is preferred over",
    "chunk_index": 57
  },
  {
    "chunk_id": "doc4_58",
    "file_name": "doc4.pdf",
    "text": " the data type and the underlying embedding model to ensure effective \nand accurate retrieval. \n \nQ49. Explain why cosine similarity is preferred over other distance metrics \nin RAG retrieval. \n \nCosine similarity is preferred in RAG retrieval because it measures the angle between \nvectors, focusing on their direction (meaning) rather than magnitude. This makes it \n18                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \neffective for textual data where the meaning lies more in the direction of the embedding \nthan its length.  \n \nUnlike Euclidean distance or dot product, cosine similarity is invariant to vector lengt",
    "chunk_index": 58
  },
  {
    "chunk_id": "doc4_59",
    "file_name": "doc4.pdf",
    "text": " more in the direction of the embedding \nthan its length.  \n \nUnlike Euclidean distance or dot product, cosine similarity is invariant to vector length, \nproviding stable and interpretable similarity scores. This helps RAG systems retrieve \nrelevant document chunks even when text lengths vary, improving accuracy and \nconsistency in semantic search. \n \nQ50. Compare keyword-based retrieval and semantic retrieval in RAG \nsystems. \n \nKeyword-based retrieval in RAG systems relies on the exact or partial matching of \nkeywords in a query to fetch relevant document chunks. This  offers high precision for \nqueries with specific terms but may miss semantically related information. In contrast, \nsemantic retrieval uses embeddings to understand the meaning behind the query and \nretrieves conceptually ",
    "chunk_index": 59
  },
  {
    "chunk_id": "doc4_60",
    "file_name": "doc4.pdf",
    "text": "ntically related information. In contrast, \nsemantic retrieval uses embeddings to understand the meaning behind the query and \nretrieves conceptually relevant content even when keywords differ. \n \nCombining both methods can balance precision and semantic understanding for \neffective retrieval in RAG systems. \n \nQ51. How does hybrid search work in the context of RAG retrieval? \n \nHybrid search in RAG systems combines keyword-based retrieval and semantic vector \nsearch to leverage the strengths of both methods. It uses a weighted formula to balance \nkeyword relevance and semantic similarity scores. This allows precise matching on exact \nterms while also capturing conceptually related content.  \n \nQ52. When do you opt for hybrid search instead of semantic search? \n \nHybrid search is preferred",
    "chunk_index": 60
  },
  {
    "chunk_id": "doc4_61",
    "file_name": "doc4.pdf",
    "text": "hile also capturing conceptually related content.  \n \nQ52. When do you opt for hybrid search instead of semantic search? \n \nHybrid search is preferred over pure semantic search when there is a need to balance \nexact keyword matches with semantic understanding, especially in scenarios where \nusers require both precision and contextual relevance.  \n \nIt is ideal for domains where queries may include specific terms, codes, or entities that \nmust be matched exactly, while also benefiting from capturing synonyms or related \nconcepts.  \n \nQ53. How do you balance relevance and diversity when retrieving \ndocument chunks for RAG? \n19                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                            ",
    "chunk_index": 61
  },
  {
    "chunk_id": "doc4_62",
    "file_name": "doc4.pdf",
    "text": "                                        Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nThe retrieval step in RAG relies on cosine similarity to identify top-k relevant document \nchunks. However, one downside of this approach is that it can return highly similar \ndocument chunks, leading to redundancy.  \n \nBalancing relevance and diversity is crucial in RAG retrieval to include contextually \nimportant yet diverse document chunks, preventing redundancy and capturing a \nbroader range of perspectives. This balance helps when dealing with complex questions, \nas different viewpoints or unique insights can improve the answer's quality while still \nbeing accurate. \n \nTech",
    "chunk_index": 62
  },
  {
    "chunk_id": "doc4_63",
    "file_name": "doc4.pdf",
    "text": "when dealing with complex questions, \nas different viewpoints or unique insights can improve the answer's quality while still \nbeing accurate. \n \nTechniques like Maximal Marginal Relevance (MMR) help to select document chunks \nthat are both highly relevant to the query and diverse from each other, reducing \nredundancy. \n \nQ54.  How do sparse embeddings differ from dense embeddings in terms of \nkeyword matching and retrieval interpretability? \n \nSparse embeddings provide interpretability and excel at exact keyword matching. These \nembeddings represent text as high-dimensional vectors with many zeros. In this, each \ndimension corresponds to a specific term or feature, making retrieval results more \nunderstandable.  \n \nIn contrast, dense embeddings are low-dimensional, continuous vectors with",
    "chunk_index": 63
  },
  {
    "chunk_id": "doc4_64",
    "file_name": "doc4.pdf",
    "text": "pecific term or feature, making retrieval results more \nunderstandable.  \n \nIn contrast, dense embeddings are low-dimensional, continuous vectors with mostly \nnon-zero values learned from neural networks, capturing semantic relationships and \ncontext beyond exact matches. This makes dense embeddings less interpretable but \nmore effective for retrieving semantically related content where keywords do not exactly \noverlap.  \n \nThus, sparse embeddings are favored for precise keyword-based retrieval and \ninterpretability, while dense embeddings support richer, context-aware retrieval. Hybrid \napproaches leverage the strengths of both sparse and dense embeddings to enhance \nretrieval performance. \n \nQ55. How can fine-tuning embedding models improve the retriever’s \nperformance in RAG? \n \n20     ",
    "chunk_index": 64
  },
  {
    "chunk_id": "doc4_65",
    "file_name": "doc4.pdf",
    "text": "nse embeddings to enhance \nretrieval performance. \n \nQ55. How can fine-tuning embedding models improve the retriever’s \nperformance in RAG? \n \n20                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nGeneral embedding models in RAG systems are trained on broad and diverse datasets \nthat capture wide-ranging language patterns. However, they often lack depth in \nvocabulary and context specific to domains. \n \nFine-tuning embedding models aligns the embedding space more closely with \ndomain-specific language and context. This allows the embedding model to better \nrepresent domain-specific terminology and jargon, which re",
    "chunk_index": 65
  },
  {
    "chunk_id": "doc4_66",
    "file_name": "doc4.pdf",
    "text": "sely with \ndomain-specific language and context. This allows the embedding model to better \nrepresent domain-specific terminology and jargon, which results in more precise and \nrelevant retrieval.  \n \nQ56. Design a retrieval strategy for a RAG system that needs to handle \nboth structured data (knowledge graphs) and unstructured data (text \ndocuments) simultaneously. \n \nA retrieval strategy for a RAG system handling both structured (knowledge graphs) and \nunstructured data (text documents) involves a hybrid approach combining vector-based \nsemantic search with graph-based retrieval techniques.  \n \nThe system first indexes unstructured text document chunks using vector embeddings \nfor semantic similarity search, while structured data from knowledge graphs is queried \nusing graph traversal me",
    "chunk_index": 66
  },
  {
    "chunk_id": "doc4_67",
    "file_name": "doc4.pdf",
    "text": "ument chunks using vector embeddings \nfor semantic similarity search, while structured data from knowledge graphs is queried \nusing graph traversal methods that leverage explicit entity relationships and schema \nmetadata.  \n \nThe results from both sources are then fused to ensure factual precision from structured \ndata and contextual richness from unstructured text. This combined approach enhances \ncompleteness and reduces hallucinations in generated responses. \n \nQ57. Discuss the strategies to scale embeddings in RAG retrieval. \n \nTo scale embeddings in RAG retrieval, strategies like Matryoshka Representation \nLearning (MRL) and quantization are highly effective. \n \nMRL enables flexible embeddings by training a single model to produce nested \nrepresentations of varying sizes, allowing tru",
    "chunk_index": 67
  },
  {
    "chunk_id": "doc4_68",
    "file_name": "doc4.pdf",
    "text": " are highly effective. \n \nMRL enables flexible embeddings by training a single model to produce nested \nrepresentations of varying sizes, allowing truncation to smaller dimensions (e.g., 64 or \n128) with minimal performance loss, achieving up to 14x size reduction and significant \nretrieval speed-ups.  \n \nQuantization reduces memory usage by compressing embeddings into lower-bit formats \nlike float8 or int8. Combining MRL with quantization can yield up to 8x compression, \noptimizing storage and retrieval efficiency while maintaining high accuracy for \nlarge-scale RAG systems. \n21                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com           ",
    "chunk_index": 68
  },
  {
    "chunk_id": "doc4_69",
    "file_name": "doc4.pdf",
    "text": " KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \n \nQ58. What advantages does quantization offer over dimensionality \nreduction for scaling embeddings? \n \nQuantization offers several advantages over dimensionality reduction for scaling \nembeddings in RAG retrieval. It compresses embeddings by reducing the precision of \nnumerical values (e.g., from float32 to int8 or float8), achieving up to 4x storage \nreduction with minimal performance loss.  \n \nUnlike dimensionality reduction, which may discard important features and degrade \naccuracy, quantization preserves the full dimensionality of embeddings, maintaining \nricher semantic information. Additionally, quantization accele",
    "chunk_index": 69
  },
  {
    "chunk_id": "doc4_70",
    "file_name": "doc4.pdf",
    "text": "e \naccuracy, quantization preserves the full dimensionality of embeddings, maintaining \nricher semantic information. Additionally, quantization accelerates computation on \nhardware optimized for lower-precision formats, improving retrieval speed.  \n \nThis makes it particularly effective for large-scale RAG systems where storage and \nlatency are critical, while dimensionality reduction risks compromising retrieval quality. \n \nQ59. Explain the pros and cons of quantized embeddings in RAG retrieval. \n \nQuantized embeddings in RAG systems offer significant benefits such as drastically \nreduced memory requirements and much faster retrieval speeds. This makes RAG \nretrieval more efficient and scalable when dealing with large knowledge bases.  \n \nHowever, the trade-off is a slight drop in retriev",
    "chunk_index": 70
  },
  {
    "chunk_id": "doc4_71",
    "file_name": "doc4.pdf",
    "text": " This makes RAG \nretrieval more efficient and scalable when dealing with large knowledge bases.  \n \nHowever, the trade-off is a slight drop in retrieval accuracy or relevance. Additionally, \nquantization effectiveness can vary depending on the embedding model.  \n \nOverall, quantized embeddings enable cost-effective, high-speed retrieval but require \nmanaging a controlled trade-off between resource savings and accuracy. \n \nQ60. Compare scalar and binary quantization for embeddings in RAG \nretrieval. \n \nScalar quantization in RAG retrieval compresses embeddings by reducing the bit \nprecision (commonly to int8), offering a moderate 4x reduction in memory usage while \nmaintaining a good balance between retrieval accuracy and speed.  \n \nBinary quantization, on the other hand, converts embedding",
    "chunk_index": 71
  },
  {
    "chunk_id": "doc4_72",
    "file_name": "doc4.pdf",
    "text": "in memory usage while \nmaintaining a good balance between retrieval accuracy and speed.  \n \nBinary quantization, on the other hand, converts embeddings to 1-bit vectors, achieving \nup to 32x compression and significantly faster retrieval but at the cost of greater \naccuracy loss.   \n22                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nOverall, scalar quantization suits use cases prioritizing accuracy with some compression, \nwhile binary quantization excels in large-scale, speed-critical scenarios where maximal \nmemory efficiency outweighs some loss of precision. \n                                   \n \n \n        ",
    "chunk_index": 72
  },
  {
    "chunk_id": "doc4_73",
    "file_name": "doc4.pdf",
    "text": "e-scale, speed-critical scenarios where maximal \nmemory efficiency outweighs some loss of precision. \n                                   \n \n \n                     🚀 AIxFunda Newsletter (free) \nJoin 🚀 AIxFunda free newsletter to get the latest updates and interesting \ntutorials related to Generative AI, LLMs, Agents and RAG. \n \n✨ Weekly GenAI updates. \n📄 Weekly LLM, Agents and RAG paper updates. \n📝 1 fresh blog post on an interesting topic every week. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n23                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nQ61. How does re-ranking differ from the initial retrieval proc",
    "chunk_index": 73
  },
  {
    "chunk_id": "doc4_74",
    "file_name": "doc4.pdf",
    "text": "                                    aixfunda.substack.com                         \n \n \nQ61. How does re-ranking differ from the initial retrieval process in RAG? \n \nThe initial retrieval process typically uses a bi-encoder that encodes queries and \ndocuments independently and then fetches a broad set of candidates quickly.  \n \nThe re-ranking process reorders the retrieval results by taking the query and each \nretrieved document chunk as a single combined input, scoring their relevance through \ndeep interaction. This improves the final ranking quality at the cost of higher \ncomputational overhead \n \nThis two-stage approach balances efficiency and accuracy by separating fast, broad \nretrieval from slower, more exact reranking. \n \nQ62. Explain the pros and cons of using re-rankers in RAG. \n \n",
    "chunk_index": 74
  },
  {
    "chunk_id": "doc4_75",
    "file_name": "doc4.pdf",
    "text": "and accuracy by separating fast, broad \nretrieval from slower, more exact reranking. \n \nQ62. Explain the pros and cons of using re-rankers in RAG. \n \nRe-rankers reorder search results by taking the query and each retrieved document \nchunk as a single combined input, scoring their relevance through deep interaction \nwithin one model pass. This helps to prioritize the most relevant information in the \nlimited context windows in LLMs. \n \nHowever, re-rankers introduce increased latency and higher computational costs since \nthey perform deep, query-chunk interaction at query time, making them less suitable for \nreal-time or high-traffic applications. \n \nThe trade-off between enhanced precision and increased costs makes re-rankers ideal \nfor specialized use cases but less suitable for applicatio",
    "chunk_index": 75
  },
  {
    "chunk_id": "doc4_76",
    "file_name": "doc4.pdf",
    "text": "ns. \n \nThe trade-off between enhanced precision and increased costs makes re-rankers ideal \nfor specialized use cases but less suitable for applications prioritizing speed and \ncost-efficiency. \n \nQ63. What are the different types of re-ranker models that can be used in \nRAG? \n \nThe different types of re-ranker models used in Retrieval-Augmented Generation (RAG) \nare \n \nCross-Encoder Rerankers: These models jointly encode the query and document chunk \npair to produce a highly accurate relevance score, offering nuanced understanding of \nrelationships but with medium computational cost. \n \n24                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com",
    "chunk_index": 76
  },
  {
    "chunk_id": "doc4_77",
    "file_name": "doc4.pdf",
    "text": "     Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nMulti-Vector or Late Interaction Models: Such as ColBERT, they encode queries and \ndocument chunks separately but perform fine-grained interaction later, balancing \nefficiency and performance with lower cost. \n \nLarge Language Model (LLM) Rerankers: Utilize powerful LLMs to reason about \nquery-document chunk relevance, achieving great accuracy but incurring high \ncomputational overhead. \n \nThese models vary in their performance and computational cost, and choice depends on \nthe application's accuracy and latency requirements. \n \nQ64. Compare general re-rankers and instruction-following re-rankers in \nRAG. \n \nGenera",
    "chunk_index": 77
  },
  {
    "chunk_id": "doc4_78",
    "file_name": "doc4.pdf",
    "text": "ds on \nthe application's accuracy and latency requirements. \n \nQ64. Compare general re-rankers and instruction-following re-rankers in \nRAG. \n \nGeneral re-rankers in RAG systems primarily focus on re-ranking retrieved document \nchunks just based on their semantic relevance to the user query.  \n \nIn contrast, instruction-following re-rankers go a step further by dynamically adjusting \nrankings based on additional user-provided instructions such as document recency, \nsource reliability, or metadata criteria.  \n \nQ65. Why is the cross-encoder typically used as the re-ranker rather than \nthe bi-encoder? \n \nThe cross-encoder is typically used as the re-ranker rather than the bi-encoder because \nit processes the query and candidate document chunks together, allowing it to capture \nintricate cont",
    "chunk_index": 78
  },
  {
    "chunk_id": "doc4_79",
    "file_name": "doc4.pdf",
    "text": "he re-ranker rather than the bi-encoder because \nit processes the query and candidate document chunks together, allowing it to capture \nintricate contextual interactions and provide more accurate relevance scores.  \n \nWhile bi-encoders encode queries and document chunks separately, enabling fast and \nscalable retrieval of broad candidate sets, they miss detailed relationships between \nquery-document chunk pairs.  \n \nCross-encoders, though slower and more resource-intensive, excel in precision, making \nthem well-suited for re-ranking a small set of top candidates identified by the \nbi-encoder. This combined approach balances scalability with accuracy, leveraging \nbi-encoders for efficient candidate retrieval and cross-encoders for refined final ranking. \n \n \n25                              ",
    "chunk_index": 79
  },
  {
    "chunk_id": "doc4_80",
    "file_name": "doc4.pdf",
    "text": "ccuracy, leveraging \nbi-encoders for efficient candidate retrieval and cross-encoders for refined final ranking. \n \n \n25                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ66.  A RAG system retrieves 20 candidate document chunks but can only \nfit 5 in the LLM's context window. Without re-ranking, how might this \nlimitation affect response quality, and what specific problems would a \nre-ranker solve? \n \nWhen a RAG system retrieves 20 candidate document chunks but can only fit 5 in the \nLLM's context window, the limitation can cause the model to miss critical information \nfrom the discarded document chunks. Without",
    "chunk_index": 80
  },
  {
    "chunk_id": "doc4_81",
    "file_name": "doc4.pdf",
    "text": " only fit 5 in the \nLLM's context window, the limitation can cause the model to miss critical information \nfrom the discarded document chunks. Without re-ranking, the top 5 document chunks \nmay not be the most relevant, leading to incomplete or less accurate answers.  \n \nA re-ranker solves this by analyzing and scoring all retrieved document chunks based on \nrelevance and contextual alignment with the query, ensuring the most relevant chunks \nare included in the limited window.  \n \nThis filtering reduces retrieval noise, enhances coherence, and maximizes the usefulness \nof the input for the generative model, thereby improving the overall quality of the \nresponse. \n \n \nQ67. Describe a scenario where a BM25 retrieval might return relevant \nchunks but in poor ranking order. How would a neural",
    "chunk_index": 81
  },
  {
    "chunk_id": "doc4_82",
    "file_name": "doc4.pdf",
    "text": "ity of the \nresponse. \n \n \nQ67. Describe a scenario where a BM25 retrieval might return relevant \nchunks but in poor ranking order. How would a neural re-ranker \nspecifically address this limitation? \n \nA typical scenario where BM25 retrieval yields relevant document chunks but in poor \nranking order arises when the query uses synonyms or phrases that vary from those in \nthe documents. This is because BM25’s exact keyword matching may surface all relevant \nitems, but fail to prioritize those most contextually aligned due to its lack of semantic \nunderstanding.  \n \nFor instance, searching for \"car maintenance\" might retrieve document chunks about \n\"vehicle upkeep\" and \"automobile servicing,\" but BM25 may rank less relevant \ndocument chunks higher if they have keyword overlaps rather than se",
    "chunk_index": 82
  },
  {
    "chunk_id": "doc4_83",
    "file_name": "doc4.pdf",
    "text": "out \n\"vehicle upkeep\" and \"automobile servicing,\" but BM25 may rank less relevant \ndocument chunks higher if they have keyword overlaps rather than semantic closeness. \nNeural re-rankers explicitly address this by leveraging deep contextual and semantic \nsignals, reordering the candidate set to prioritize document chunks that best match the \nquery’s intent and meaning. \n \n26                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ68. If your RAG system serves both simple factual queries and complex \nanalytical questions, how would you decide when to bypass the re-ranker \nfor efficiency while maintaining quality? \n \nTo",
    "chunk_index": 83
  },
  {
    "chunk_id": "doc4_84",
    "file_name": "doc4.pdf",
    "text": " factual queries and complex \nanalytical questions, how would you decide when to bypass the re-ranker \nfor efficiency while maintaining quality? \n \nTo decide when to bypass the re-ranker in a RAG system, queries should be classified \nbased on complexity. Simple factual queries like \"What is the capital of France?\"  \nrequire straightforward and well-known answers. Re-ranker can be skipped for simple \nfactual queries, as the initial retrieval is likely to yield highly relevant results.  \n \nFor complex analytical questions, such as those requiring synthesis or reasoning across \nmultiple chunks, the re-ranker should be used to ensure the most relevant chunks are \nprioritized. \n \nQ69. Describe the vector pre-computation and storage strategy in a \nbi-encoder + cross-encoder pipeline. Why can't c",
    "chunk_index": 84
  },
  {
    "chunk_id": "doc4_85",
    "file_name": "doc4.pdf",
    "text": "evant chunks are \nprioritized. \n \nQ69. Describe the vector pre-computation and storage strategy in a \nbi-encoder + cross-encoder pipeline. Why can't cross-encoders pre-compute \ntext representations like bi-encoders can? \n \nThe RAG pipeline leverages bi-encoders for fast retrieval and cross-encoders for the \nprecise reranking of top candidates. \n \nBi-encoders pre-compute chunk representations by encoding them into fixed-size dense \nvectors offline and then storing them in a vector database for efficient retrieval.  \n \nCross-encoders, however, cannot pre-compute chunk representations because they \njointly encode query-chunk pairs, capturing intricate interactions through attention \nmechanisms, requiring both inputs at inference time to produce a relevance score.  \n \n \nQ70. Compare the noise ",
    "chunk_index": 85
  },
  {
    "chunk_id": "doc4_86",
    "file_name": "doc4.pdf",
    "text": "ricate interactions through attention \nmechanisms, requiring both inputs at inference time to produce a relevance score.  \n \n \nQ70. Compare the noise reduction capabilities of re-rankers versus simply \nincreasing the similarity threshold in initial retrieval. When would each \napproach be more appropriate? \n \nIncreasing the similarity threshold in initial retrieval reduces noise by filtering out less \nsimilar chunks but risks missing relevant ones due to embedding limitations.  \nRe-rankers reduce noise by prioritizing relevant chunks by deeply understanding \nquery-chunk relevance. \n \nThe choice depends on the trade-off between computational cost and precision \nrequirements. Re-rankers are preferred for high-stakes applications like legal or medical \n27                                       ",
    "chunk_index": 86
  },
  {
    "chunk_id": "doc4_87",
    "file_name": "doc4.pdf",
    "text": "st and precision \nrequirements. Re-rankers are preferred for high-stakes applications like legal or medical \n27                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nsearches.  \nIncreasing the similarity threshold is simpler and faster, suitable for \nresource-constrained environments. \n \nQ71. What challenges do re-rankers face regarding computational \noverhead and latency?  \n \nRe-rankers in RAG systems face significant challenges related to increased \ncomputational overhead and latency, as each query-chunk pair must be processed.  This \nlatency increase can hinder high-throughput environments, making re-rankers \ncomp",
    "chunk_index": 87
  },
  {
    "chunk_id": "doc4_88",
    "file_name": "doc4.pdf",
    "text": "head and latency, as each query-chunk pair must be processed.  This \nlatency increase can hinder high-throughput environments, making re-rankers \ncomputationally expensive compared to initial vector searches and limiting scalability. \n \n \nQ72. In real-time applications with strict latency requirements, describe \ntwo specific optimization strategies you could implement to reduce \nre-ranking overhead while preserving most of the quality gains. \n \nTwo effective strategies to reduce re-ranking overhead while preserving quality gains in \nreal-time RAG applications are  \n \n1) Query classifier: Deploy a query classifier to identify complex or analytical queries, \ninvoking the re-ranker only for these while bypassing it for simple factual queries.  \n \n2) Model distillation: Train a smaller, faster",
    "chunk_index": 88
  },
  {
    "chunk_id": "doc4_89",
    "file_name": "doc4.pdf",
    "text": "ical queries, \ninvoking the re-ranker only for these while bypassing it for simple factual queries.  \n \n2) Model distillation: Train a smaller, faster re-ranking model to mimic the behavior of a \nlarger, more accurate model, enabling quicker inference with minimal quality loss.  \n \nThese approaches balance latency and quality by minimizing computational load \nwithout significantly compromising the relevance of retrieved results. \n \n \nQ73. How would you evaluate the effectiveness of a reranker in a RAG \nsystem? Which metrics (e.g., MRR, MAP, NDCG) would you prioritize and \nwhy? \n \nThe effectiveness of a re-ranker in a RAG system is best evaluated using ranking metrics \nthat capture how well it prioritizes relevant chunks. Mean Reciprocal Rank (MRR) is key \nwhen the focus is on how quickly t",
    "chunk_index": 89
  },
  {
    "chunk_id": "doc4_90",
    "file_name": "doc4.pdf",
    "text": "ted using ranking metrics \nthat capture how well it prioritizes relevant chunks. Mean Reciprocal Rank (MRR) is key \nwhen the focus is on how quickly the first relevant chunk appears, ideal for \nquestion-answering scenarios.  \n \n28                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nMean Average Precision (MAP) is useful when multiple relevant chunks matter, \nmeasuring both precision and ranking quality across results. Normalized Discounted \nCumulative Gain (NDCG) excels when relevance is graded rather than binary, rewarding \nthe correct order of highly relevant chunks.  \n \n \n                                   LLM S",
    "chunk_index": 90
  },
  {
    "chunk_id": "doc4_91",
    "file_name": "doc4.pdf",
    "text": "ls when relevance is graded rather than binary, rewarding \nthe correct order of highly relevant chunks.  \n \n \n                                   LLM Survey Papers Collection \n \nThis repo is highly useful to stay updated with LLM research.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n29                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ74. Explain the difference between Precision@k and Recall@k in the \ncontext of RAG. When might you prefer one over the other? \n \nPrecision@k focuses on accuracy of the retrieval by measuring the proportion of the \ntop-k retrieved chunks that are relevant to the query.  \nRecall@k focuses on \ncomplet",
    "chunk_index": 91
  },
  {
    "chunk_id": "doc4_92",
    "file_name": "doc4.pdf",
    "text": "on accuracy of the retrieval by measuring the proportion of the \ntop-k retrieved chunks that are relevant to the query.  \nRecall@k focuses on \ncompleteness of the retrieval by measuring the proportion of all relevant chunks that are \nretrieved within the top-k results. \n \nYou might choose Precision@k when you want to ensure high-quality, relevant chunks \nto reduce noise. On the other hand, you might choose Recall@k when it is crucial to \ncapture as many relevant chunks as possible. \n \nQ75. Why is MRR unsuitable when there are multiple relevant chunks per \nquery, and how does MAP address this limitation? \n \nMRR (Mean Reciprocal Rank) considers the rank of the first relevant chunk and \ndisregards the ranks and presence of other relevant chunks. This limitation makes MRR \nmore appropriate for",
    "chunk_index": 92
  },
  {
    "chunk_id": "doc4_93",
    "file_name": "doc4.pdf",
    "text": " the rank of the first relevant chunk and \ndisregards the ranks and presence of other relevant chunks. This limitation makes MRR \nmore appropriate for scenarios where a single chunk sufficiently answers the query.  \n \nIn contrast, MAP (Mean Average Precision) addresses this by averaging the precision \nacross all relevant ranks, accounting for the presence and order of all relevant chunks. \nHence, MAP is preferred over MRR for cases where multiple relevant chunks contribute \nto answering a query comprehensively. \n \nQ76. Given a retrieval result, show how to manually calculate the MAP@5 \n(Mean Average Precision at 5). What does MAP reveal about the retrieval \nsystem that raw Precision does not? \n \nTo manually calculate MAP@5, list the top 5 retrieved items for each query and note the \npositi",
    "chunk_index": 93
  },
  {
    "chunk_id": "doc4_94",
    "file_name": "doc4.pdf",
    "text": " the retrieval \nsystem that raw Precision does not? \n \nTo manually calculate MAP@5, list the top 5 retrieved items for each query and note the \npositions where relevant items appear; then, compute precision at each relevant \nposition (e.g., if the first relevant item appears at rank 2, precision = 1/2) and average \nthese values to get the Average Precision (AP) for that query. Repeat this for all queries \nand take the mean of their APs for MAP@5. \n \nMAP@5 reveals a retrieval system's ability to rank relevant items higher. In contrast, \nraw Precision only measures the proportion of relevant items retrieved, ignoring their \norder. This makes MAP@5 a better indicator of how well the system prioritizes \nrelevance at the top of the result list. \n \n30                                             ",
    "chunk_index": 94
  },
  {
    "chunk_id": "doc4_95",
    "file_name": "doc4.pdf",
    "text": "P@5 a better indicator of how well the system prioritizes \nrelevance at the top of the result list. \n \n30                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nQ77. If all the relevant chunks are at the very bottom, how would \nthis affect MRR, MAP, and NDCG metrics? Explain each. \n \nIf all relevant chunks are at the bottom of a ranked list for a search query, MRR (Mean \nReciprocal Rank) would be low, as it measures the reciprocal of the rank of the first \nrelevant chunk. MAP (Mean Average Precision) would also be low, as it averages \nprecision across all relevant chunks, penalizing late appearances heavily due to \n",
    "chunk_index": 95
  },
  {
    "chunk_id": "doc4_96",
    "file_name": "doc4.pdf",
    "text": "nk. MAP (Mean Average Precision) would also be low, as it averages \nprecision across all relevant chunks, penalizing late appearances heavily due to \nincreasing denominators in precision calculations.  \n \nNDCG (Normalized Discounted Cumulative Gain) would similarly be low, as it discounts \nthe relevance scores of chunks appearing later in the ranking, reducing the cumulative \ngain.  \n \nQ78. Suppose your RAG retriever gets perfect Recall@10 but low \nPrecision@10. What problems could this cause for the downstream \ngenerator? \n \nPerfect Recall@10 means all relevant chunks are retrieved within the top 10 results. Low \nPrecision@10 indicates many of those retrieved chunks are irrelevant. If a RAG retriever \nachieves perfect Recall@10 but low Precision@10, the downstream generator receives \nall ",
    "chunk_index": 96
  },
  {
    "chunk_id": "doc4_97",
    "file_name": "doc4.pdf",
    "text": "of those retrieved chunks are irrelevant. If a RAG retriever \nachieves perfect Recall@10 but low Precision@10, the downstream generator receives \nall the relevant information mixed with much irrelevant content.  \n \nThis will confuse the generator model and increase the chance of generating off-topic or \ninaccurate responses.  \n \nQ79. Compare and contrast “order-aware” and “order-unaware” retrieval \nmetrics in RAG, giving examples for each from the set (Precision, Recall, \nMRR, MAP, NDCG). \n \nOrder-aware retrieval metrics consider the ranking of retrieved items, emphasizing the \nimportance of higher-ranked relevant results. For example, Mean Reciprocal Rank \n(MRR) and Normalized Discounted Cumulative Gain (NDCG) are order-aware, as MRR \nevaluates the rank of the first relevant item and NDCG",
    "chunk_index": 97
  },
  {
    "chunk_id": "doc4_98",
    "file_name": "doc4.pdf",
    "text": "eciprocal Rank \n(MRR) and Normalized Discounted Cumulative Gain (NDCG) are order-aware, as MRR \nevaluates the rank of the first relevant item and NDCG accounts for relevance scores and \nranking positions. \n \nOrder-unaware metrics focus solely on whether relevant items are retrieved and ignore \nthe order.  Precision and Recall are order-unaware, measuring the proportion of \n31                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nrelevant items retrieved (Precision) and the proportion of relevant items found out of all \nrelevant items (Recall), without considering their order.  \n \nQ80. How would the value of NDCG@k ch",
    "chunk_index": 98
  },
  {
    "chunk_id": "doc4_99",
    "file_name": "doc4.pdf",
    "text": "the proportion of relevant items found out of all \nrelevant items (Recall), without considering their order.  \n \nQ80. How would the value of NDCG@k change if all relevant chunks are \nretrieved but in the reverse order (least to most relevant)? \n \nNDCG@k rewards placing highly relevant chunks at earlier ranks and applies a \nlogarithmic discount to relevance scores at lower positions. So reversing the order \npushes the most relevant chunks further down the list—making them less valuable in \nthe NDCG calculation.  \n \nWhile all relevant items are present, their suboptimal positions reduce the overall score \nsince NDCG is sensitive to both the presence and order of relevant items in the top k. \nThe value of NDCG@k will decrease compared to the ideal ranking, but will remain \nhigher than a ranki",
    "chunk_index": 99
  },
  {
    "chunk_id": "doc4_100",
    "file_name": "doc4.pdf",
    "text": "sence and order of relevant items in the top k. \nThe value of NDCG@k will decrease compared to the ideal ranking, but will remain \nhigher than a ranking with irrelevant chunks at the top.  \n \nQ81. What is the significance of Context Precision@K in evaluating a RAG \nretriever, and how does it differ from standard Precision@k in traditional \ninformation retrieval? \n \nThe standard Precision@k in traditional information retrieval just measures the \nproportion of relevant items among the top-k results and ignores the order. Unlike \nstandard Precision@k,  Context Precision@K not only checks whether relevant chunks \nare retrieved, but also whether they appear at higher ranks in the context.  \n \nContext Precision@K ensures useful information is prioritized in the retrieved context \nwhich greatly i",
    "chunk_index": 100
  },
  {
    "chunk_id": "doc4_101",
    "file_name": "doc4.pdf",
    "text": "hey appear at higher ranks in the context.  \n \nContext Precision@K ensures useful information is prioritized in the retrieved context \nwhich greatly impacts the quality of the generated answer. \n  \nQ82. Why does Context Precision@K use a weighted sum approach with \nrelevance indicators, and how does this better reflect RAG retriever \nperformance? \n \nContext Precision is computed as the weighted sum of Precision@k, normalized by the \nnumber of relevant chunks.  Here the weighted sum accounts for both the presence and \nthe rank of relevant chunks in the retrieved context. By multiplying Precision@k with \nthe relevance indicator at each position, the metric rewards cases where relevant \ninformation appears earlier, reflecting the importance of ranking quality.  \n \n32                          ",
    "chunk_index": 101
  },
  {
    "chunk_id": "doc4_102",
    "file_name": "doc4.pdf",
    "text": ", the metric rewards cases where relevant \ninformation appears earlier, reflecting the importance of ranking quality.  \n \n32                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nThis approach better evaluates RAG retrievers, since in generative settings, not just \nretrieving relevant chunks but placing them at higher ranks significantly impacts the \nmodel’s ability to produce accurate answers. \n \nQ83. Given a retrieval result where relevant chunks appear at positions 2, \n4, 6, and 8 out of 10 total chunks, manually calculate the Context \nPrecision@10. What does this score tell us about the retriever's ranking \nabili",
    "chunk_index": 102
  },
  {
    "chunk_id": "doc4_103",
    "file_name": "doc4.pdf",
    "text": ", \n4, 6, and 8 out of 10 total chunks, manually calculate the Context \nPrecision@10. What does this score tell us about the retriever's ranking \nability? \n \nTo calculate Context Precision@10 with relevant chunks at positions 2, 4, 6, and 8, first \nassign relevance indicators v_k=1 at these ranks and 0 elsewhere. Calculate \nPrecision@k at each relevant rank: at 2, Precision@2 = 1/2 = 0.5; at 4, Precision@4 = \n2/4 = 0.5; at 6, Precision@6 = 3/6 = 0.5; at 8, Precision@8 = 4/8 = 0.5. The weighted \nsum is  0.5+0.5+0.5+0.5=2.  \n \nDividing the weighted sum by the total number of relevant chunks (4) gives Context \nPrecision@10 = 0.5. This score indicates the retriever has moderate ranking ability, \nretrieving relevant chunks but not consistently ranking them at the very top, thus \nlimiting optimal",
    "chunk_index": 103
  },
  {
    "chunk_id": "doc4_104",
    "file_name": "doc4.pdf",
    "text": "ates the retriever has moderate ranking ability, \nretrieving relevant chunks but not consistently ranking them at the very top, thus \nlimiting optimal prioritization of useful context. \n \nQ84. A RAG system achieves Context Precision@5 = 0.8. What are the \npossible scenarios that could lead to this score? \n \nA Context Precision@5 score of 0.8 in a RAG system indicates that not all of the top five \nretrieved chunks are relevant to the ground truth. This could occur if, for instance, four \nout of five chunks are relevant (v_k = 1) and one is irrelevant (v_k = 0), leading to a \nlower weighted sum of Precision@k when normalized by the total number of relevant \nchunks.  \n \nAnother scenario might involve three relevant chunks and two irrelevant ones, with the \nrelevant chunks ranked higher but st",
    "chunk_index": 104
  },
  {
    "chunk_id": "doc4_105",
    "file_name": "doc4.pdf",
    "text": " of relevant \nchunks.  \n \nAnother scenario might involve three relevant chunks and two irrelevant ones, with the \nrelevant chunks ranked higher but still resulting in a score less than 1 due to the \npresence of irrelevant chunks.  \n \nQ85. Explain the possible reasons for a RAG retrieval system with \nconsistently low context precision.  \n \nContext Precision is computed as the weighted sum of Precision@k, normalized by the \nnumber of relevant chunks. So low Context Precision@k scores reflect the presence of \n33                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nhigh proportion of irrelevant chunks or poor ranking of",
    "chunk_index": 105
  },
  {
    "chunk_id": "doc4_106",
    "file_name": "doc4.pdf",
    "text": "                                              aixfunda.substack.com                         \n \nhigh proportion of irrelevant chunks or poor ranking of relevant chunks within the top K \nresults.  \n \nThis could stem from ineffective query understanding, where the system misinterprets \nthe user’s intent, or a poorly designed retrieval algorithm that fails to prioritize chunks \nmatching the ground truth. Additionally, a noisy or low-quality document corpus might \ncontain few relevant chunks, causing irrelevant ones to dominate the retrieved set.  \n \nQ86. Compare Context Recall with traditional information retrieval recall. \nWhy is Context Recall computed using \"ground truth claims\" rather than \nsimply counting relevant documents? \n \nContext Recall in RAG retrieval differs from traditional info",
    "chunk_index": 106
  },
  {
    "chunk_id": "doc4_107",
    "file_name": "doc4.pdf",
    "text": "computed using \"ground truth claims\" rather than \nsimply counting relevant documents? \n \nContext Recall in RAG retrieval differs from traditional information retrieval recall by \nfocusing on the completeness of information through ground truth claims rather than \nmerely counting relevant documents.  \n \nWhile traditional recall counts how many relevant documents are retrieved, Context \nRecall decomposes the reference answer into individual claims and checks if these \nspecific claims are found in the retrieved context.  \n \nThis approach ensures a more fine-grained evaluation of whether all necessary pieces of \ninformation required to answer the query are present in the context or not.  \n \n \nQ87. What does context precision measure in a RAG retriever, and how \ndoes it differ from context reca",
    "chunk_index": 107
  },
  {
    "chunk_id": "doc4_108",
    "file_name": "doc4.pdf",
    "text": " query are present in the context or not.  \n \n \nQ87. What does context precision measure in a RAG retriever, and how \ndoes it differ from context recall? \n \nContext Precision in a RAG retriever measures how well the system ranks relevant \nchunks of information higher than irrelevant ones within the retrieved context, \nemphasizing the prioritization of useful data. In contrast, Context Recall assesses the \ncompleteness of the retrieved context, evaluating whether all the relevant pieces of \ninformation necessary to answer the query are present.  \n \nTogether, they provide complementary insights: context precision ensures useful \ninformation is prioritized, whereas context recall ensures that no important information \nis missed. \n \nQ88. In a RAG pipeline, how might context recall impact the c",
    "chunk_index": 108
  },
  {
    "chunk_id": "doc4_109",
    "file_name": "doc4.pdf",
    "text": "prioritized, whereas context recall ensures that no important information \nis missed. \n \nQ88. In a RAG pipeline, how might context recall impact the completeness \nof generated answers? Describe a scenario illustrating this relationship. \n34                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nContext Recall in a RAG pipeline directly impacts the completeness of generated \nanswers by measuring how well the retriever gathers all relevant pieces of information \nrequired to answer the user query. \n \nFor instance, if a question about a historical event requires multiple claims or facts, a \nlow Context Recall score indi",
    "chunk_index": 109
  },
  {
    "chunk_id": "doc4_110",
    "file_name": "doc4.pdf",
    "text": " to answer the user query. \n \nFor instance, if a question about a historical event requires multiple claims or facts, a \nlow Context Recall score indicates that some key facts were missed in the retrieved \ncontext, leading to incomplete answers.  \n \nA high Context Recall ensures the generator (LLM) has access to all necessary \ninformation to produce a complete and well-informed response.  \n \nQ89. If your retriever achieves high context precision but low context \nrecall, what types of user queries would likely suffer most? \n \nIf a retriever in a RAG pipeline achieves high context precision but low context recall, \nuser queries that require multiple distinct pieces of information or comprehensive \ncoverage are likely to suffer most.  \n \nQueries, like complex multi-fact questions or those nee",
    "chunk_index": 110
  },
  {
    "chunk_id": "doc4_111",
    "file_name": "doc4.pdf",
    "text": "tiple distinct pieces of information or comprehensive \ncoverage are likely to suffer most.  \n \nQueries, like complex multi-fact questions or those needing extensive context to answer \nfully, will suffer because despite the retrieved chunks being relevant (high precision), \nmany essential relevant chunks are missing overall (low recall).  \n \nThis results in incomplete answers, as important claims or facts are absent, limiting the \nmodel’s ability to generate a thorough response.  \n \nQ90. In what situations would you prioritize Context Precision over \nContext Recall in a RAG retriever, and how would this impact the \ngenerator’s performance? \n \nIn situations where precision is critical, such as in high-risk domains like healthcare, \nfinance, or legal applications, prioritizing Context Precisi",
    "chunk_index": 111
  },
  {
    "chunk_id": "doc4_112",
    "file_name": "doc4.pdf",
    "text": "In situations where precision is critical, such as in high-risk domains like healthcare, \nfinance, or legal applications, prioritizing Context Precision over Context Recall in a \nRAG retriever is essential. This ensures that only the most relevant and trustworthy \ninformation is retrieved, minimizing the risk of including irrelevant or misleading \ncontent that could negatively impact the generator's response.  \n \nWhile this may limit the breadth of information (lower recall), it improves the quality \nand reliability of the generated answers by reducing noise.  \n \n35                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         ",
    "chunk_index": 112
  },
  {
    "chunk_id": "doc4_113",
    "file_name": "doc4.pdf",
    "text": " Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ91. Describe a scenario where a RAG system might achieve high Context \nRecall but still produce poor answers. What complementary metrics would \nyou use alongside Context Recall to get a complete picture of retriever \nperformance? \n \nA RAG system might achieve high Context Recall by retrieving most or all relevant \ninformation pieces but still produce poor answers if the retrieved context contains noisy \nor irrelevant data that confuses the generator.  \n \nTo get a complete picture of retriever performance, complementary metrics like Context \nPrecision should be used alongside Context Recall. Context Recall along with Context \nPrecision  en",
    "chunk_index": 113
  },
  {
    "chunk_id": "doc4_114",
    "file_name": "doc4.pdf",
    "text": "r performance, complementary metrics like Context \nPrecision should be used alongside Context Recall. Context Recall along with Context \nPrecision  ensures retrieved content is not only comprehensive but is also relevant and \nwell-ranked. \n \nQ92. If your RAG retriever consistently shows Context Recall scores below \n0.6, what are the three potential root causes? \n \nContext Recall scores below 0.6 mean that the retriever is missing a significant portion \nof the relevant information required to answer user queries.  \n \nThe three potential root causes are  \n(1) an incomplete or outdated knowledge base lacking necessary information,  \n(2) ineffective embedding model or ranking algorithms causing semantically relevant \nchunks to be missed, and  \n(3) poor chunking strategy leading to loss of key ",
    "chunk_index": 114
  },
  {
    "chunk_id": "doc4_115",
    "file_name": "doc4.pdf",
    "text": "ive embedding model or ranking algorithms causing semantically relevant \nchunks to be missed, and  \n(3) poor chunking strategy leading to loss of key information.  \n \nQ93. Why is it important for RAG systems to optimize both context \nprecision and context recall simultaneously? What trade-offs might occur? \n \nIt is important for RAG systems to optimize both context precision and context recall \nsimultaneously. This is because context precision ensures that the retrieved information \nis highly relevant and ranked appropriately. The Context Recall metric ensures that all \nnecessary information is included in the retrieved context so that the generator can \noutput a complete answer.  \n \nThe trade-off often arises because increasing recall by retrieving more chunks may \nintroduce irrelevant ch",
    "chunk_index": 115
  },
  {
    "chunk_id": "doc4_116",
    "file_name": "doc4.pdf",
    "text": "nerator can \noutput a complete answer.  \n \nThe trade-off often arises because increasing recall by retrieving more chunks may \nintroduce irrelevant chunks, lowering precision. At the same time, focusing solely on \nprecision might omit important information, leading to incomplete responses.  \n \n36                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nBalancing these metrics helps create a RAG system that retrieves relevant content \nefficiently while covering the query comprehensively, resulting in accurate and thorough \ngenerated answers. \n \nQ94. Explain why Context Relevancy is considered a \"reference-free\" metric \nw",
    "chunk_index": 116
  },
  {
    "chunk_id": "doc4_117",
    "file_name": "doc4.pdf",
    "text": "mprehensively, resulting in accurate and thorough \ngenerated answers. \n \nQ94. Explain why Context Relevancy is considered a \"reference-free\" metric \nwhile Context Precision and Context Recall are \"reference-dependent.\" \nWhen would you prefer using Context Relevancy over the other two \nmetrics? \n \nContext Relevancy is considered a \"reference-free\" metric because it evaluates how \nrelevant the retrieved context is to the user’s query without needing a reference answer. \nIt measures the proportion of statements in the retrieved context that are relevant to the \nquery.  \n \nIn contrast, Context Precision and Context Recall are \"reference-dependent\" as they \nrequire a reference answer to determine relevance and completeness of retrieval.  \n \nContext Relevancy is preferred when reference answers ",
    "chunk_index": 117
  },
  {
    "chunk_id": "doc4_118",
    "file_name": "doc4.pdf",
    "text": "\" as they \nrequire a reference answer to determine relevance and completeness of retrieval.  \n \nContext Relevancy is preferred when reference answers are unavailable. The Context \nRelevancy metric offers a way to assess retrieval quality based solely on the query and \nretrieved context itself. This is useful for real-time scenarios where ground truth may \nnot exist. \n \n \nQ95. Describe a scenario where a RAG retriever achieves high Context \nRelevancy but low Context Precision. What does this imply about the \nretriever’s performance? \n \nA RAG retriever achieves high Context Relevancy but low Context Precision when it \nretrieves a context where most statements are relevant to the user’s query, but the \nrelevant chunks are ranked lower in the retrieved list, overshadowed by irrelevant ones.  \n",
    "chunk_index": 118
  },
  {
    "chunk_id": "doc4_119",
    "file_name": "doc4.pdf",
    "text": "most statements are relevant to the user’s query, but the \nrelevant chunks are ranked lower in the retrieved list, overshadowed by irrelevant ones.  \n \nFor example, if a query about \"machine learning algorithms\" retrieves a context with \nmany relevant statements but places them after less relevant or noisy chunks, Context \nRelevancy is high (most statements are query-related), but Context Precision@K is low \ndue to poor ranking of relevant chunks.  \n \nThis implies the retriever is effective at fetching relevant content but struggles to \nprioritize relevant chunks over irrelevant ones.  \n \n37                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.co",
    "chunk_index": 119
  },
  {
    "chunk_id": "doc4_120",
    "file_name": "doc4.pdf",
    "text": "      Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \n \nQ96. Suppose a RAG retriever retrieves all relevant chunks but includes \nmany irrelevant ones, leading to low Context Relevancy. How would you \nimprove the retriever to address this issue? \n \nA RAG retriever retrieving all relevant chunks along with many irrelevant ones results in \nlow context relevancy scores. This can be addressed by improving the retriever by \nrefining its filtering and ranking mechanisms. Techniques such as enhancing \nembedding model quality, applying stricter similarity thresholds, or integrating a \nre-ranking model can help prioritize highly relevant chunks and suppress noise.  \n \nAddition",
    "chunk_index": 120
  },
  {
    "chunk_id": "doc4_121",
    "file_name": "doc4.pdf",
    "text": "pplying stricter similarity thresholds, or integrating a \nre-ranking model can help prioritize highly relevant chunks and suppress noise.  \n \nAdditionally, improving the chunking strategy to create more precise and semantically \ncoherent chunks can reduce irrelevant retrievals. These optimizations ensure retrieved \ncontext is both comprehensive and focused on the most relevant information. \n \nQ97. How does the Faithfulness metric assess the quality of a RAG \ngenerator? \n \nThe Faithfulness metric assesses the quality of a RAG generator by measuring how \nfactually consistent the generated response is with the retrieved context. It is computed \nas the ratio of claims in the response that are supported by the retrieved context to the \ntotal number of claims.   \n \nA score of 1 indicates all cla",
    "chunk_index": 121
  },
  {
    "chunk_id": "doc4_122",
    "file_name": "doc4.pdf",
    "text": "as the ratio of claims in the response that are supported by the retrieved context to the \ntotal number of claims.   \n \nA score of 1 indicates all claims are fully supported, reflecting high factual accuracy. A \nscore of 0 shows no claims are supported, indicating complete factual inconsistency. \nThis metric ensures the RAG system generates reliable and contextually grounded \nresponses. \n \nQ98. Distinguish between Faithfulness and Context Precision metrics in RAG \nevaluation. Why might a system have high Context Precision but low \nFaithfulness, and what would this indicate about your pipeline? \n \nFaithfulness measures how factually consistent a RAG generator’s response is with the \nretrieved context. This metric is computed as the ratio of supported claims to total \nclaims in the response.",
    "chunk_index": 122
  },
  {
    "chunk_id": "doc4_123",
    "file_name": "doc4.pdf",
    "text": " a RAG generator’s response is with the \nretrieved context. This metric is computed as the ratio of supported claims to total \nclaims in the response. The Context Precision metric focuses on the prioritization of \nrelevant information by evaluating how well a retriever ranks relevant chunks within the \ntop K.  \n \n38                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nA system might have high Context Precision but low Faithfulness if the retriever \neffectively ranks relevant chunks highly, but the generator introduces unsupported or \ncontradictory claims not grounded in the context. This indicates a strong retrieval",
    "chunk_index": 123
  },
  {
    "chunk_id": "doc4_124",
    "file_name": "doc4.pdf",
    "text": "levant chunks highly, but the generator introduces unsupported or \ncontradictory claims not grounded in the context. This indicates a strong retrieval stage \nbut a flawed generation stage, where the model fails to accurately interpret or utilize the \nretrieved information. \n \nQ99. A RAG system has high context precision but low faithfulness. How \nwould you address this? \n \nA RAG system with high context precision and low faithfulness happens when the \nretriever is selecting relevant chunks accurately, but the generator is producing \nresponses with unsupported claims. To address this, one should focus on improving the \ngenerator’s grounding and claim verification processes. \n \nUse stronger cross-checking mechanisms like natural language inference models or \nfact-checking modules against the",
    "chunk_index": 124
  },
  {
    "chunk_id": "doc4_125",
    "file_name": "doc4.pdf",
    "text": "d claim verification processes. \n \nUse stronger cross-checking mechanisms like natural language inference models or \nfact-checking modules against the retrieved context. Additionally, tuning the generation \nprompts to encourage reliance on the context can help increase faithfulness.  \n \nQ100. Why might a RAG system with perfect Context Recall still fail to \nproduce accurate responses? How does the Faithfulness metric help \ndiagnose this issue? \n \nContext recall evaluates the completeness of the retrieved context in a RAG pipeline. \nPerfect Context Recall means the retrieved context includes all the ground truth claims.  \nA RAG system with perfect Context Recall may still fail to produce accurate responses. \nThis happens when the generator hallucinates and  includes claims unsupported by th",
    "chunk_index": 125
  },
  {
    "chunk_id": "doc4_126",
    "file_name": "doc4.pdf",
    "text": "fect Context Recall may still fail to produce accurate responses. \nThis happens when the generator hallucinates and  includes claims unsupported by the \nretrieved context.  \n \nThe Faithfulness metric helps diagnose this issue by measuring how many claims in the \ngenerated response are factually supported by the retrieved context. A low or moderate \nfaithfulness metric score indicates a less accurate response, i.e., the response includes \nunsupported claims.   \n \nQ101. \nExplain \nhow \nhallucinations \nin \nLLMs \nspecifically \nimpact the \nFaithfulness metric. What techniques could you implement to improve the \nFaithfulness metric score? \n \nThe faithfulness metric measures the proportion of claims in the response that are \nbacked up by context.  \nHallucinations reduce the faithfulness metric sco",
    "chunk_index": 126
  },
  {
    "chunk_id": "doc4_127",
    "file_name": "doc4.pdf",
    "text": "thfulness metric measures the proportion of claims in the response that are \nbacked up by context.  \nHallucinations reduce the faithfulness metric score by \n39                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nintroducing unsupported claims in the generated response. These unsupported claims \neither contradict or have no basis in the provided context. \n \nTo improve Faithfulness scores, techniques such as incorporating natural language \ninference (NLI) or fact-checking models to verify claims, using prompt engineering to \ndiscourage unsupported generation, etc., can be used. \n \nQ102. How does Response Relevancy di",
    "chunk_index": 127
  },
  {
    "chunk_id": "doc4_128",
    "file_name": "doc4.pdf",
    "text": "ng models to verify claims, using prompt engineering to \ndiscourage unsupported generation, etc., can be used. \n \nQ102. How does Response Relevancy differ from Context Relevancy, and \nwhy do you need both metrics to properly evaluate a RAG system?  \n \nResponse Relevancy measures how well a RAG system's generated response aligns with \nthe user’s query by calculating the ratio of relevant statements in the response to the \ntotal statements. Context Relevancy evaluates the relevance of retrieved context to the \nquery by measuring the proportion of relevant statements in the context.  \n \nBoth metrics are essential because Context Relevancy ensures the retriever fetches \nrelevant context, while Response Relevancy verifies that the generator produces an \nanswer directly addressing the query.  \n ",
    "chunk_index": 128
  },
  {
    "chunk_id": "doc4_129",
    "file_name": "doc4.pdf",
    "text": "s the retriever fetches \nrelevant context, while Response Relevancy verifies that the generator produces an \nanswer directly addressing the query.  \n \nA RAG system could retrieve relevant context but generate an off-topic response, or vice \nversa. Hence, evaluating both ensures the entire RAG pipeline—retrieval and \ngeneration—performs effectively.  \n \nQ103. The generator’s response mentions facts not present in the retrieved \ncontext. Describe how faithfulness and response relevancy metrics would \nbe impacted. \n \nThe faithfulness metric measures the proportion of claims in the response that are \nbacked up by context. Therefore, the score will decrease if the LLM-generated answer \ncontains unsupported claims. \n \nIn the case of the Response Relevancy metric, the score will decrease only if ",
    "chunk_index": 129
  },
  {
    "chunk_id": "doc4_130",
    "file_name": "doc4.pdf",
    "text": "l decrease if the LLM-generated answer \ncontains unsupported claims. \n \nIn the case of the Response Relevancy metric, the score will decrease only if the \nunsupported facts are irrelevant to the user query. Otherwise, the score will remain \nhigh. \n \nThis underscores a key difference between these two metrics: Faithfulness metric looks \nfor answer’s factual consistency with the context, while Response Relevancy assesses \nanswer’s relevancy with the query. \n \n40                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n🚀AIxFunda Newsletter                                                          aixfunda.substack.com                         \n \nQ104. How does the Response Relevancy metric help evaluate whether a \nRAG generator is addressing the use",
    "chunk_index": 130
  },
  {
    "chunk_id": "doc4_131",
    "file_name": "doc4.pdf",
    "text": "unda.substack.com                         \n \nQ104. How does the Response Relevancy metric help evaluate whether a \nRAG generator is addressing the user’s query effectively? \n \nThe Response Relevancy metric is computed as the ratio of relevant statements in the \nresponse to the total number of statements. So, this metric checks the effectiveness of \nthe RAG generator by measuring how well the response aligns with the user query. \n \nA score close to 1 means the answer directly addresses the query with little to no \nirrelevant content. A score close to 0 means that the answer contains information that is \nnot related to the question.   \n \nQ105. When evaluating RAG generator output, what are the risks of relying \nsolely on response relevancy? How can including the faithfulness metric \nimprove ",
    "chunk_index": 131
  },
  {
    "chunk_id": "doc4_132",
    "file_name": "doc4.pdf",
    "text": "When evaluating RAG generator output, what are the risks of relying \nsolely on response relevancy? How can including the faithfulness metric \nimprove reliability? \n \nThe Response Relevancy metric tells you how relevant the answer is to the user query. \nBut this metric doesn't check if the answer is based on the retrieved context, so it misses \nfactual errors. \n \nThe faithfulness metric is the number of supported claims divided by the total number \nof claims. Adding the faithfulness metric makes the system more reliable by making sure \nthat the claims in the LLM-generated response are supported by the context. \n   \nThis dual evaluation ensures the RAG system delivers both relevant and factual \nresponses, reducing the risk of misleading outputs. \n \n \n \n \n \n \n \n \n \n \n41                       ",
    "chunk_index": 132
  },
  {
    "chunk_id": "doc4_133",
    "file_name": "doc4.pdf",
    "text": " the RAG system delivers both relevant and factual \nresponses, reducing the risk of misleading outputs. \n \n \n \n \n \n \n \n \n \n \n41                                                          Kalyan KS (Follow on Twitter and LinkedIn) \n \n",
    "chunk_index": 133
  }
]